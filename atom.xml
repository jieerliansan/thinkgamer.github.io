<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>机器学习／python／云计算</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-12T17:01:59.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据结构算法之链表</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E9%93%BE%E8%A1%A8/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之链表/</id>
    <published>2017-11-12T16:58:37.000Z</published>
    <updated>2017-11-12T17:01:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>链表面试总结，使用python实现，参考：<a href="https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html" target="_blank" rel="external">https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html</a><br><a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line"></span><br><span class="line"># 定义链表</span><br><span class="line">class ListNode:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.data = None</span><br><span class="line">        self.pnext = None</span><br><span class="line"></span><br><span class="line"># 链表操作类</span><br><span class="line">class ListNode_handle:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.cur_node = None</span><br><span class="line">    </span><br><span class="line">    # 链表添加元素</span><br><span class="line">    def add(self,data):</span><br><span class="line">        ln = ListNode()</span><br><span class="line">        ln.data = data</span><br><span class="line">        </span><br><span class="line">        ln.pnext = self.cur_node</span><br><span class="line">        self.cur_node = ln</span><br><span class="line">        return ln</span><br><span class="line">    </span><br><span class="line">    # 打印链表</span><br><span class="line">    def prt(self,ln):</span><br><span class="line">        while ln:</span><br><span class="line">            print(ln.data,end=&quot;  &quot;)</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">    # 逆序输出</span><br><span class="line">    def _reverse(self,ln):</span><br><span class="line">        _list = []</span><br><span class="line">        while ln:</span><br><span class="line">            _list.append(ln.data)</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        ln_2 = ListNode()</span><br><span class="line">        ln_h = ListNode_handle()</span><br><span class="line">        for i in _list:</span><br><span class="line">            ln_2 = ln_h.add(i)</span><br><span class="line">        return ln_2</span><br><span class="line">    </span><br><span class="line">    # 求链表的长度</span><br><span class="line">    def _length(self,ln):</span><br><span class="line">        _len = 0</span><br><span class="line">        while ln:</span><br><span class="line">            _len += 1</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        return _len</span><br><span class="line">    </span><br><span class="line">    # 查找指定位置的节点</span><br><span class="line">    def _find_loc(self,ln,loc):</span><br><span class="line">        _sum = 0</span><br><span class="line">        while ln and _sum != loc:</span><br><span class="line">            _sum += 1</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        return ln.data</span><br><span class="line">    </span><br><span class="line">    # 判断某个节点是否在链表中</span><br><span class="line">    def _exist(self,ln,data):</span><br><span class="line">        flag = False</span><br><span class="line">        while ln and data != ln.data:</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        return flag</span><br><span class="line"></span><br><span class="line"># 创建链表   </span><br><span class="line">ln = ListNode()</span><br><span class="line">ln_h = ListNode_handle()</span><br><span class="line">a = [1,4,2,5,8,5,7,9]</span><br><span class="line">for i in a:</span><br><span class="line">    ln = ln_h.add(i)</span><br><span class="line"></span><br><span class="line">print(&quot;正序输出...&quot;)</span><br><span class="line">ln_h.prt(ln)</span><br><span class="line"></span><br><span class="line">print(&quot;\n\n逆序输出...&quot;)</span><br><span class="line">ln_2 = ln_h._reverse(ln)</span><br><span class="line">ln_h.prt(ln_2)</span><br><span class="line"></span><br><span class="line"># 求链表ln的长度</span><br><span class="line">length = ln_h._length(ln)</span><br><span class="line">print(&quot;\n\nln的长度为:&quot;,length)</span><br><span class="line"></span><br><span class="line"># 查找链表ln中的倒数第３个节点</span><br><span class="line">data = ln_h._find_loc(ln,ln_h._length(ln)-3)</span><br><span class="line">print(&quot;\n\n倒数第三个节点为:&quot;,data)</span><br><span class="line"></span><br><span class="line"># 返回某个节点在链表中的位置</span><br><span class="line">loc = ln_h._loc(ln,5)</span><br><span class="line"></span><br><span class="line">#　判断某个节点是否在链表中</span><br><span class="line">flag = ln_h._exist(ln,5)</span><br><span class="line">print(&quot;\n\n５是否存在与链表ln中:&quot;,end=&quot; &quot;)</span><br><span class="line">if flag:</span><br><span class="line">    print(&quot;Yes&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;No&quot;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;链表面试总结，使用python实现，参考：&lt;a href=&quot;https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="链表" scheme="http://yoursite.com/tags/%E9%93%BE%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>数据结构算法之合并两个有序序列</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%BA%8F%E5%88%97/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之合并两个有序序列/</id>
    <published>2017-11-12T16:55:29.000Z</published>
    <updated>2017-11-12T16:57:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>有序序列的合并，python实现。<br><a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line"></span><br><span class="line">a = [2,4,6,8,10]</span><br><span class="line">b = [3,5,7,9,11,13,15]</span><br><span class="line">c = []</span><br><span class="line"></span><br><span class="line">def merge(a,b):</span><br><span class="line">    i,j = 0,0</span><br><span class="line">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class="line">        if a[i]&lt;b[j]:</span><br><span class="line">            c.append(a[i])</span><br><span class="line">            i+=1</span><br><span class="line">        else:</span><br><span class="line">            c.append(b[j])</span><br><span class="line">            j+=1</span><br><span class="line">    if i&lt;=len(a)-1:</span><br><span class="line">        for m in a[i:]:</span><br><span class="line">            c.append(m)</span><br><span class="line">    </span><br><span class="line">    if j&lt;=len(b)-1:</span><br><span class="line">        for n in b[j:]:</span><br><span class="line">            c.append(n)</span><br><span class="line">    print(c)</span><br><span class="line"></span><br><span class="line">merge(a,b)</span><br></pre></td></tr></table></figure><p>运行结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有序序列的合并，python实现。&lt;br&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="排序" scheme="http://yoursite.com/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>数据结构算法之排序</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之排序/</id>
    <published>2017-11-12T16:51:28.000Z</published>
    <updated>2017-11-12T16:54:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。</p><a id="more"></a><h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#coding：utf-8</span><br><span class="line"></span><br><span class="line"># 冒泡排序</span><br><span class="line">def maopao():</span><br><span class="line">    a = [2,1,4,3,9,5,6,8,7]</span><br><span class="line">    for i in range(len(a)-1):</span><br><span class="line">        for j in range(len(a)-1-i):</span><br><span class="line">            if a[j]&gt;a[j+1]:</span><br><span class="line">                temp = a[j]</span><br><span class="line">                a[j] = a[j+1]</span><br><span class="line">                a[j+1] = temp</span><br><span class="line">    print(a)</span><br><span class="line">maopao()</span><br></pre></td></tr></table></figure><p>结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p><hr><h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 归并排序</span><br><span class="line">def merge(a,b):</span><br><span class="line">    i,j = 0,0</span><br><span class="line">    c = []</span><br><span class="line">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class="line">        if a[i]&lt;b[j]:</span><br><span class="line">            c.append(a[i])</span><br><span class="line">            i+=1</span><br><span class="line">        else:</span><br><span class="line">            c.append(b[j])</span><br><span class="line">            j+=1</span><br><span class="line">    if i&lt;=len(a)-1:</span><br><span class="line">        for m in a[i:]:</span><br><span class="line">            c.append(m)</span><br><span class="line">    </span><br><span class="line">    if j&lt;=len(b)-1:</span><br><span class="line">        for n in b[j:]:</span><br><span class="line">            c.append(n)</span><br><span class="line">    return c</span><br><span class="line"></span><br><span class="line">def guibing(a):</span><br><span class="line">    if len(a)&lt;=1:</span><br><span class="line">        return a</span><br><span class="line">    center = int(len(a)/2)</span><br><span class="line">    left = guibing(a[:center])</span><br><span class="line">    right = guibing(a[center:])</span><br><span class="line">    return merge(left,right)</span><br><span class="line"></span><br><span class="line">print(guibing([2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure><p>结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p><hr><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="　快速排序"></a>　快速排序</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#　快速排序    </span><br><span class="line">def kpsort(left,right,a):</span><br><span class="line">    based = a[left]</span><br><span class="line">    i = left</span><br><span class="line">    j = right</span><br><span class="line">    while i &lt; j:</span><br><span class="line">        # 从数组右边开始遍历</span><br><span class="line">        while a[j]&gt;=based and i&lt;j:</span><br><span class="line">            j -= 1</span><br><span class="line">        a[i] = a[j]</span><br><span class="line">        while a[i]&lt;=based and i&lt;j:</span><br><span class="line">            i += 1</span><br><span class="line">        </span><br><span class="line">        a[j]= a[i]</span><br><span class="line">        a[i] = based</span><br><span class="line"></span><br><span class="line">    return i</span><br><span class="line">    </span><br><span class="line">def kuaipai(left,right,a):</span><br><span class="line">    if left&lt;right:</span><br><span class="line">        p = kpsort(left,right,a)</span><br><span class="line">        kuaipai(left,p-1,a)</span><br><span class="line">        kuaipai(p+1,right,a)</span><br><span class="line"></span><br><span class="line">    return a</span><br><span class="line">            </span><br><span class="line">print(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure><p>结果为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。&lt;/p&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="排序" scheme="http://yoursite.com/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>数据结构算法之二叉树</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之二叉树/</id>
    <published>2017-11-12T16:44:41.000Z</published>
    <updated>2017-11-12T16:50:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。</p><a id="more"></a><p>注意：py2中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print root.elem,</span><br></pre></td></tr></table></figure></p><p>在py3中要换成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print (root.elem,end=&quot;  &quot;)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line"># 定义节点类</span><br><span class="line">class Node:</span><br><span class="line">    def __init__(self,elem = -1,):</span><br><span class="line">        self.elem = elem</span><br><span class="line">        self.left = None</span><br><span class="line">        self.right = None</span><br><span class="line">        </span><br><span class="line"># 定义二叉树</span><br><span class="line">class Tree:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.root = Node()</span><br><span class="line">        self.myqu = []</span><br><span class="line">    </span><br><span class="line">    # 添加节点</span><br><span class="line">    def add(self,elem):</span><br><span class="line">        node = Node(elem)</span><br><span class="line">        if self.root.elem == -1:         # 判断如果是根节点</span><br><span class="line">            self.root  = node</span><br><span class="line">            self.myqu.append(self.root)</span><br><span class="line">        else:</span><br><span class="line">            treenode = self.myqu[0]</span><br><span class="line">            if treenode.left == None:</span><br><span class="line">                treenode.left = node</span><br><span class="line">                self.myqu.append(treenode.left)</span><br><span class="line">            else:</span><br><span class="line">                treenode.right = node</span><br><span class="line">                self.myqu.append(treenode.right)</span><br><span class="line">                self.myqu.pop(0)</span><br><span class="line">        </span><br><span class="line">    # 利用递归实现树的先序遍历</span><br><span class="line">    def xianxu(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return</span><br><span class="line">        print root.elem,</span><br><span class="line">        self.xianxu(root.left)</span><br><span class="line">        self.xianxu(root.right)</span><br><span class="line">        </span><br><span class="line">    # 利用递归实现树的中序遍历</span><br><span class="line">    def zhongxu(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return </span><br><span class="line">        self.zhongxu(root.left)</span><br><span class="line">        print root.elem,</span><br><span class="line">        self.zhongxu(root.right)</span><br><span class="line">        </span><br><span class="line">    # 利用递归实现树的后序遍历</span><br><span class="line">    def houxu(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return </span><br><span class="line">        self.houxu(root.left)</span><br><span class="line">        self.houxu(root.right)</span><br><span class="line">        print root.elem,</span><br><span class="line">    </span><br><span class="line">    # 利用队列实现层次遍历</span><br><span class="line">    def cengci(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return</span><br><span class="line">        myq = []</span><br><span class="line">        node = root</span><br><span class="line">        myq.append(node)</span><br><span class="line">        while myq:</span><br><span class="line">            node = myq.pop(0)</span><br><span class="line">            print node.elem,</span><br><span class="line">            if node.left != None:</span><br><span class="line">                myq.append(node.left)</span><br><span class="line">            if node.right != None:</span><br><span class="line">                myq.append(node.right)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"># 创建一个树，添加节点</span><br><span class="line">tree = Tree()</span><br><span class="line">for i in range(10):</span><br><span class="line">    tree.add(i)</span><br><span class="line">    </span><br><span class="line">print(&quot;二叉树的先序遍历:&quot;)</span><br><span class="line">print(tree.xianxu(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;二叉树的中序遍历:&quot;)</span><br><span class="line">print(tree.zhongxu(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;二叉树的后序遍历:&quot;)</span><br><span class="line">print(tree.houxu(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;二叉树的层次遍历&quot;)</span><br><span class="line">print(tree.cengci(tree.root))</span><br></pre></td></tr></table></figure><p>运行结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">二叉树的先序遍历:</span><br><span class="line">0 1 3 7 8 4 9 2 5 6 None</span><br><span class="line">二叉树的中序遍历:</span><br><span class="line">7 3 8 1 9 4 0 5 2 6 None</span><br><span class="line">二叉树的后序遍历:</span><br><span class="line">7 8 3 9 4 1 5 6 2 0 None</span><br><span class="line">二叉树的层次遍历</span><br><span class="line">0 1 2 3 4 5 6 7 8 9 None</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。&lt;/p&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="二叉树" scheme="http://yoursite.com/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>回归分析之Sklearn实现电力预测</title>
    <link href="http://yoursite.com/2017/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E4%B9%8BSklearn%E5%AE%9E%E7%8E%B0%E7%94%B5%E5%8A%9B%E9%A2%84%E6%B5%8B/"/>
    <id>http://yoursite.com/2017/11/07/机器学习/回归分析之Sklearn实现电力预测/</id>
    <published>2017-11-07T05:39:15.000Z</published>
    <updated>2017-11-11T09:49:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>参考原文：<a href="http://www.cnblogs.com/pinard/p/6016029.html" target="_blank" rel="external">http://www.cnblogs.com/pinard/p/6016029.html</a><br>这里进行了手动实现，增强记忆。<br><a id="more"></a></p><h1 id="1：数据集介绍"><a href="#1：数据集介绍" class="headerlink" title="1：数据集介绍"></a>1：数据集介绍</h1><p>使用的数据是UCI大学公开的机器学习数据</p><p>数据的介绍在这： <a href="http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant</a></p><p>数据的下载地址在这：<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/00294/" target="_blank" rel="external">http://archive.ics.uci.edu/ml/machine-learning-databases/00294/</a></p><p>里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。</p><p>我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:</p><script type="math/tex; mode=display">PE = \theta _{0} + \theta _{0} * AT + \theta _{0} * V +\theta _{0} * AP +\theta _{0}*RH</script><p>而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。</p><hr><h1 id="2：准备数据"><a href="#2：准备数据" class="headerlink" title="2：准备数据"></a>2：准备数据</h1><p>下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理</p><p>sklearn的归一化处理参考：<a href="http://blog.csdn.net/gamer_gyt/article/details/77761884" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/77761884</a></p><hr><h1 id="3：使用pandas来进行数据的读取"><a href="#3：使用pandas来进行数据的读取" class="headerlink" title="3：使用pandas来进行数据的读取"></a>3：使用pandas来进行数据的读取</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"># pandas 读取数据</span><br><span class="line">data = pd.read_csv(&quot;Folds5x2_pp.csv&quot;)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>然后会看到如下结果，说明数据读取成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ATVAPRHPE</span><br><span class="line">08.3440.771010.8490.01480.48</span><br><span class="line">123.6458.491011.4074.20445.75</span><br><span class="line">229.7456.901007.1541.91438.76</span><br><span class="line">319.0749.691007.2276.79453.09</span><br><span class="line">411.8040.661017.1397.20464.43</span><br></pre></td></tr></table></figure><hr><h1 id="4：准备运行算法的数据"><a href="#4：准备运行算法的数据" class="headerlink" title="4：准备运行算法的数据"></a>4：准备运行算法的数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = data[[&quot;AT&quot;,&quot;V&quot;,&quot;AP&quot;,&quot;RH&quot;]]</span><br><span class="line">print X.shape</span><br><span class="line">y = data[[&quot;PE&quot;]]</span><br><span class="line">print y.shape</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(9568, 4)</span><br><span class="line">(9568, 1)</span><br></pre></td></tr></table></figure><p>说明有9658条数据，其中”AT”,”V”,”AP”,”RH” 四列作为样本特征，”PE”列作为样本输出。</p><hr><h1 id="5：划分训练集和测试集"><a href="#5：划分训练集和测试集" class="headerlink" title="5：划分训练集和测试集"></a>5：划分训练集和测试集</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line"></span><br><span class="line"># 划分训练集和测试集</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)</span><br><span class="line">print X_train.shape</span><br><span class="line">print y_train.shape</span><br><span class="line">print X_test.shape</span><br><span class="line">print y_test.shape</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(7176, 4)</span><br><span class="line">(7176, 1)</span><br><span class="line">(2392, 4)</span><br><span class="line">(2392, 1)</span><br></pre></td></tr></table></figure><p>75%的数据被划分为训练集，25的数据划分为测试集。</p><hr><h1 id="6：运行sklearn-线性模型"><a href="#6：运行sklearn-线性模型" class="headerlink" title="6：运行sklearn 线性模型"></a>6：运行sklearn 线性模型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">linreg = LinearRegression()</span><br><span class="line">linreg.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># 训练模型完毕，查看结果</span><br><span class="line">print linreg.intercept_</span><br><span class="line">print linreg.coef_</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 447.06297099]</span><br><span class="line">[[-1.97376045 -0.23229086  0.0693515  -0.15806957]]</span><br></pre></td></tr></table></figure><p>即我们得到的模型结果为：</p><script type="math/tex; mode=display">PE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH</script><hr><h1 id="7：模型评价"><a href="#7：模型评价" class="headerlink" title="7：模型评价"></a>7：模型评价</h1><p>我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred = linreg.predict(X_test)</span><br><span class="line">from sklearn import metrics</span><br><span class="line"></span><br><span class="line"># 使用sklearn来计算mse和Rmse</span><br><span class="line">print &quot;MSE:&quot;,metrics.mean_squared_error(y_test, y_pred)</span><br><span class="line">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y_test, y_pred))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MSE: 20.0804012021</span><br><span class="line">RMSE: 4.48111606657</span><br></pre></td></tr></table></figure><p>得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。</p><hr><h1 id="8：交叉验证"><a href="#8：交叉验证" class="headerlink" title="8：交叉验证"></a>8：交叉验证</h1><p>我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 交叉验证</span><br><span class="line">from sklearn.model_selection import cross_val_predict</span><br><span class="line">predicted = cross_val_predict(linreg,X,y,cv=10)</span><br><span class="line">print &quot;MSE:&quot;,metrics.mean_squared_error(y, predicted)</span><br><span class="line">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y, predicted))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MSE: 20.7955974619</span><br><span class="line">RMSE: 4.56021901469</span><br></pre></td></tr></table></figure><p>可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。</p><hr><h1 id="9：画图查看结果"><a href="#9：画图查看结果" class="headerlink" title="9：画图查看结果"></a>9：画图查看结果</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 画图查看结果</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(y, predicted)</span><br><span class="line">ax.plot([y.min(), y.max()], [y.min(), y.max()], &apos;k--&apos;, lw=4)</span><br><span class="line">ax.set_xlabel(&apos;Measured&apos;)</span><br><span class="line">ax.set_ylabel(&apos;Predicted&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://img.blog.csdn.net/20171107133222238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考原文：&lt;a href=&quot;http://www.cnblogs.com/pinard/p/6016029.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/pinard/p/6016029.html&lt;/a&gt;&lt;br&gt;这里进行了手动实现，增强记忆。&lt;br&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="回归分析" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
      <category term="交叉验证" scheme="http://yoursite.com/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>回归分析之线性回归（N元线性回归）</title>
    <link href="http://yoursite.com/2017/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88N%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/</id>
    <published>2017-09-29T08:45:14.000Z</published>
    <updated>2017-11-11T09:48:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中我们介绍了 <a href="http://blog.csdn.net/gamer_gyt/article/details/78008144" target="_blank" rel="external"> 回归分析之理论篇</a>，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。</p><a id="more"></a><h1 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h1><p>预测房价：<br>输入编号    | 平方米    | 价格<br>-|-|-<br>1 |    150 |    6450<br>2    | 200    | 7450<br>3|    250    |8450<br>4|    300    |9450<br>5|    350    |11450<br>6|    400    |15450<br>7|    600|    18450</p><p>针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为</p><script type="math/tex; mode=display">H(x) = k*x + b</script><p>其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：</p><script type="math/tex; mode=display">k =\frac{ \sum_{1}^{n} (x_{i} - \bar{x} )(y_{i} - \bar{y}) } { \sum_{1}^{n}(x_{i}-\bar{x})^{2} }</script><p>自己使用python代码实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def leastsq(x,y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x,y分别是要拟合的数据的自变量列表和因变量列表</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值</span><br><span class="line">    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值</span><br><span class="line"></span><br><span class="line">    xSum = 0.0</span><br><span class="line">    ySum = 0.0</span><br><span class="line"></span><br><span class="line">    for i in range(len(x)):</span><br><span class="line">        xSum += (x[i] - meanX) * (y[i] - meanY)</span><br><span class="line">        ySum += (x[i] - meanX) ** 2</span><br><span class="line"></span><br><span class="line">    k = ySum/xSum</span><br><span class="line">    b = ySum - k * meanX</span><br><span class="line"></span><br><span class="line">    return k,b</span><br></pre></td></tr></table></figure></p><p>使用python的scipy包进行计算:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)</span><br><span class="line"></span><br><span class="line">from scipy.optimize import leastsq</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def fun(p, x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义想要拟合的函数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    k,b = p    # 从参数p获得拟合的参数</span><br><span class="line">    return k*x + b</span><br><span class="line"></span><br><span class="line">def err(p, x, y):</span><br><span class="line">    return fun(p,x) - y</span><br><span class="line"></span><br><span class="line">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class="line">p0 = [1,1]</span><br><span class="line"></span><br><span class="line">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class="line">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class="line">#换为numpy的类型</span><br><span class="line"></span><br><span class="line">x1 = np.array([150,200,250,300,350,400,600])</span><br><span class="line">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])</span><br><span class="line"></span><br><span class="line">xishu = leastsq(err, p0, args=(x1,y1))</span><br><span class="line"></span><br><span class="line">print xishu[0]</span><br></pre></td></tr></table></figure></p><p>当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：<a href="http://www.cnblogs.com/NanShan2016/p/5493429.html" target="_blank" rel="external">http://www.cnblogs.com/NanShan2016/p/5493429.html</a></p><h1 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h1><p>总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说</p><script type="math/tex; mode=display">y = a * x^2 + b * x + c</script><script type="math/tex; mode=display">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><script type="math/tex; mode=display">y = a * x_1^3 + b * x_1^2 + c * x_1 + d</script><p>在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例</p><script type="math/tex; mode=display">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><p>对应的python 代码是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from scipy.optimize import leastsq</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fun(p, x1, x2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义想要拟合的函数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    a,b,c,d = p    # 从参数p获得拟合的参数</span><br><span class="line">    return a * (x1**2) + b * x1 + c * x2 + d</span><br><span class="line"></span><br><span class="line">def err(p, x1, x2, y):</span><br><span class="line">    return fun(p,x1,x2) - y</span><br><span class="line"></span><br><span class="line">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class="line">p0 = [1,1,1,1]</span><br><span class="line"></span><br><span class="line">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class="line">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class="line">#换为numpy的类型</span><br><span class="line"></span><br><span class="line">x1 = np.array([150,200,250,300,350,400,600])    # 面积</span><br><span class="line">x2 = np.array([4,2,7,9,12,14,15])               # 楼层</span><br><span class="line">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米</span><br><span class="line"></span><br><span class="line">xishu = leastsq(err, p0, args=(x1,x2,y1))</span><br><span class="line"></span><br><span class="line">print xishu[0]</span><br></pre></td></tr></table></figure></p><h1 id="sklearn中的线性回归应用"><a href="#sklearn中的线性回归应用" class="headerlink" title="sklearn中的线性回归应用"></a>sklearn中的线性回归应用</h1><h2 id="普通最小二乘回归"><a href="#普通最小二乘回归" class="headerlink" title="普通最小二乘回归"></a>普通最小二乘回归</h2><p>这里我们使用的是sklearn中的linear_model来模拟<script type="math/tex">y=a * x_1 + b * x_2 + c</script></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [1]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [2]: linreg = LinearRegression()</span><br><span class="line"></span><br><span class="line">In [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])</span><br><span class="line"></span><br><span class="line">In [4]: linreg.coef_</span><br><span class="line">Out[4]: array([ 0.5,  0.5])</span><br><span class="line"></span><br><span class="line">In [5]: linreg.intercept_</span><br><span class="line">Out[5]: 1.1102230246251565e-16</span><br><span class="line"></span><br><span class="line">In [6]: linreg.predict([4,4])</span><br><span class="line">Out[6]: array([ 4.])</span><br><span class="line"></span><br><span class="line">In [7]: zip([&quot;x1&quot;,&quot;x2&quot;], linreg.coef_)</span><br><span class="line">Out[7]: [(&apos;x1&apos;, 0.5), (&apos;x2&apos;, 0.49999999999999989)]</span><br></pre></td></tr></table></figure><p>所以可得<script type="math/tex">y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16</script></p><p>linreg.coef_  为系数 a,b</p><p>linreg.intercept_ 为截距 c</p><p>缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。</p><h2 id="多项式回归：基函数扩展线性模型"><a href="#多项式回归：基函数扩展线性模型" class="headerlink" title="多项式回归：基函数扩展线性模型"></a>多项式回归：基函数扩展线性模型</h2><p>机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p><p>例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：</p><script type="math/tex; mode=display">y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}</script><p>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p><script type="math/tex; mode=display">y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2</script><p>我们发现，这仍然是一个线性模型，想象着创建一个新变量：</p><script type="math/tex; mode=display">z = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]</script><p>可以把线性回归模型写成下边这种形式：</p><script type="math/tex; mode=display">y(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}</script><p>我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p><p>使用如下代码，将二维数据进行二元转换,转换规则为：</p><script type="math/tex; mode=display">[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [15]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [16]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [17]: X = np.arange(6).reshape(3,2)</span><br><span class="line"></span><br><span class="line">In [18]: X</span><br><span class="line">Out[18]: </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [2, 3],</span><br><span class="line">       [4, 5]])</span><br><span class="line"></span><br><span class="line">In [19]: poly = PolynomialFeatures(degree=2)</span><br><span class="line"></span><br><span class="line">In [20]: poly.fit_transform(X)</span><br><span class="line">Out[20]: </span><br><span class="line">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class="line">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class="line">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure><p>验证：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [38]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [39]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [40]: from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">In [41]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [42]: </span><br><span class="line"></span><br><span class="line">In [42]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=3)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class="line"></span><br><span class="line">In [43]: model</span><br><span class="line">Out[43]: Pipeline(steps=[(&apos;poly&apos;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&apos;linear&apos;, LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])</span><br><span class="line"></span><br><span class="line">In [44]: x = np.arange(5)</span><br><span class="line"></span><br><span class="line">In [45]: y = 3 - 2 * x + x ** 2 - x ** 3</span><br><span class="line"></span><br><span class="line">In [46]: y</span><br><span class="line">Out[46]: array([  3,   1,  -5, -21, -53])</span><br><span class="line"></span><br><span class="line">In [47]: model = model.fit(x[:,np.newaxis],y)</span><br><span class="line"></span><br><span class="line">In [48]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class="line">Out[48]: array([ 3., -2.,  1., -1.])</span><br></pre></td></tr></table></figure></p><p>我们可以看出最后求出的参数和一元三次方程是一致的。</p><p>这里如果把degree改为2，y的方程也换一下，结果也是一致的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [51]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [52]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [53]: from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">In [54]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [55]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=2)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class="line"></span><br><span class="line">In [56]: x = np.arange(5)</span><br><span class="line"></span><br><span class="line">In [57]: y = 3 + 2 * x + x ** 2</span><br><span class="line"></span><br><span class="line">In [58]: model = model.fit(x[:, np.newaxis], y)</span><br><span class="line"></span><br><span class="line">In [59]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class="line">Out[59]: array([ 3., 2.,  1.])</span><br></pre></td></tr></table></figure></p><h2 id="线性回归的评测"><a href="#线性回归的评测" class="headerlink" title="线性回归的评测"></a>线性回归的评测</h2><p>在<a href="http://note.youdao.com/" target="_blank" rel="external">上一篇文章</a>中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。</p><p>这里我们定义预测值和真实值分别为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">true = [10, 5, 3, 2]</span><br><span class="line">pred = [9, 5, 5, 3]</span><br></pre></td></tr></table></figure></p><p>1: 平均绝对误差（Mean Absolute Error, MAE）</p><script type="math/tex; mode=display">\frac{1}{N}(\sum_{1}^{n} |y_i - \bar{y}|)</script><p>2: 均方误差（Mean Squared Error, MSE）</p><script type="math/tex; mode=display">\frac{1}{N}\sum_{1}^{n}(y_i - \bar{y})^2</script><p>3: 均方根误差（Root Mean Squared Error, RMSE）</p><script type="math/tex; mode=display">\frac{1}{N} \sqrt{ \sum_{1}^{n}(y_i - \bar{y})^2 }</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [80]: from sklearn import metrics</span><br><span class="line"></span><br><span class="line">In [81]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [82]: true = [10, 5, 3, 2]</span><br><span class="line"></span><br><span class="line">In [83]: pred = [9, 5, 5, 3]</span><br><span class="line"></span><br><span class="line">In [84]: print(&quot;MAE: &quot;, metrics.mean_absolute_error(true,pred))</span><br><span class="line">(&apos;MAE: &apos;, 1.0)</span><br><span class="line"></span><br><span class="line">In [85]: print(&quot;MAE By Hand: &quot;, (1+0+2+1)/4.)</span><br><span class="line">(&apos;MAE By Hand: &apos;, 1.0)</span><br><span class="line"></span><br><span class="line">In [86]: print(&quot;MSE: &quot;, metrics.mean_squared_error(true,pred))</span><br><span class="line">(&apos;MSE: &apos;, 1.5)</span><br><span class="line"></span><br><span class="line">In [87]: print(&quot;MSE By Hand: &quot;, (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)</span><br><span class="line">(&apos;MSE By Hand: &apos;, 1.5)</span><br><span class="line"></span><br><span class="line">In [88]: print(&quot;RMSE: &quot;, np.sqrt(metrics.mean_squared_error(true,pred)))</span><br><span class="line">(&apos;RMSE: &apos;, 1.2247448713915889)</span><br><span class="line"></span><br><span class="line">In [89]: print(&quot;RMSE By Hand: &quot;, np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))</span><br><span class="line">(&apos;RMSE By Hand: &apos;, 1.2247448713915889)</span><br></pre></td></tr></table></figure><hr><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇文章中我们介绍了 &lt;a href=&quot;http://blog.csdn.net/gamer_gyt/article/details/78008144&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt; 回归分析之理论篇&lt;/a&gt;，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。&lt;/p&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="回归分析" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="二元线性回归" scheme="http://yoursite.com/tags/%E4%BA%8C%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="多元线性回归" scheme="http://yoursite.com/tags/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>几种距离计算公式在数据挖掘中的应用场景分析</title>
    <link href="http://yoursite.com/2017/09/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%87%A0%E7%A7%8D%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%E5%9C%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2017/09/20/机器学习/几种距离计算公式在数据挖掘中的应用场景分析/</id>
    <published>2017-09-20T02:23:39.000Z</published>
    <updated>2017-11-12T17:06:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》</p><ul><li>曼哈顿距离</li><li>欧几里得距离</li><li>闵可夫斯基距离</li><li>皮尔逊相关系数</li><li>余弦相似度</li></ul><a id="more"></a><p>之前整理过一篇关于距离相关的文章：<a href="">机器学习算法中的距离和相似性计算公式，分析以及python实现</a></p><h1 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h1><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p><script type="math/tex; mode=display">\sqrt[p]{ \sum_{k=1}^{n} \left | x_{1k}-x_{2k} \right |^p}</script><p>其中p是一个变参数。</p><p>当p=1时，就是曼哈顿距离</p><p>当p=2时，就是欧氏距离</p><p>当p→∞时，就是切比雪夫距离</p><p>根据变参数的不同，闵氏距离可以表示一类的距离。</p><p>p值越大，单个维度的差值大小会对整体距离有更大的影响</p><h1 id="曼哈顿距离／欧几里得距离的瑕疵"><a href="#曼哈顿距离／欧几里得距离的瑕疵" class="headerlink" title="曼哈顿距离／欧几里得距离的瑕疵"></a>曼哈顿距离／欧几里得距离的瑕疵</h1><p>在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价：<br><img src="http://img.blog.csdn.net/20170920102356159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。</p><p>现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9</span><br></pre></td></tr></table></figure><p>同样使用欧式距离计算为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3</span><br></pre></td></tr></table></figure></p><p>当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。</p><hr><h1 id="用户问题／皮尔逊相关系数／分数膨胀"><a href="#用户问题／皮尔逊相关系数／分数膨胀" class="headerlink" title="用户问题／皮尔逊相关系数／分数膨胀"></a>用户问题／皮尔逊相关系数／分数膨胀</h1><h2 id="现象——用户问题"><a href="#现象——用户问题" class="headerlink" title="现象——用户问题"></a>现象——用户问题</h2><p>仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同：</p><ul><li>Bill没有打出极端的分数，都在2-4分之间</li><li>Jordyn似乎喜欢所有的乐队，打分都在4-5之间</li><li>Hailey是一个有趣的人，他的评分不是1就是4</li></ul><p>那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！</p><h2 id="解决该现象"><a href="#解决该现象" class="headerlink" title="解决该现象"></a>解决该现象</h2><p>解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）：</p><p><img src="http://img.blog.csdn.net/20170920111425007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下：<br><img src="http://img.blog.csdn.net/20170920112804525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。</p><p>皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。</p><p>皮尔逊相关系数的计算公式为：<br><img src="http://img.blog.csdn.net/20170920114015874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值：<br><img src="http://img.blog.csdn.net/20170920114326883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><hr><h1 id="余弦相似度／稀疏数据"><a href="#余弦相似度／稀疏数据" class="headerlink" title="余弦相似度／稀疏数据"></a>余弦相似度／稀疏数据</h1><p>假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下：</p><ul><li>如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数</li><li>如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离</li><li>如果数据是稀疏的，就使用余弦相似度</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;曼哈顿距离&lt;/li&gt;
&lt;li&gt;欧几里得距离&lt;/li&gt;
&lt;li&gt;闵可夫斯基距离&lt;/li&gt;
&lt;li&gt;皮尔逊相关系数&lt;/li&gt;
&lt;li&gt;余弦相似度&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="相似度计算" scheme="http://yoursite.com/tags/%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>回归分析之理论篇</title>
    <link href="http://yoursite.com/2017/09/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E4%B9%8B%E7%90%86%E8%AE%BA%E7%AF%87/"/>
    <id>http://yoursite.com/2017/09/17/机器学习/回归分析之理论篇/</id>
    <published>2017-09-17T00:10:27.000Z</published>
    <updated>2017-11-11T09:49:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。</p><p>CSDN链接：<a href="http://blog.csdn.net/gamer_gyt/article/details/78008144" target="_blank" rel="external">点击阅读</a><br><a id="more"></a></p><h1 id="一：一些名词定义"><a href="#一：一些名词定义" class="headerlink" title="一：一些名词定义"></a>一：一些名词定义</h1><h2 id="1）指数分布族"><a href="#1）指数分布族" class="headerlink" title="1）指数分布族"></a>1）指数分布族</h2><p>指数分布族是指可以表示为指数形式的概率分布。</p><script type="math/tex; mode=display">f_X(x\mid\theta) = h(x) \exp \left (\eta(\theta) \cdot T(x) -A(\theta)\right )</script><p>其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。</p><p>伯努利分布与高斯分布是两个典型的指数分布族</p><h3 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h3><p>又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则<br>其概率质量函数为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;\displaystyle f_&#123;X&#125;(x)=p^&#123;x&#125;(1-p)^&#123;1-x&#125;=\left\&#123;&#123;\begin&#123;matrix&#125;p&amp;&#123;\mbox&#123;if &#125;&#125;x=1,\\q\ &amp;&#123;\mbox&#123;if &#125;&#125;x=0.\\\end&#123;matrix&#125;&#125;\right.&#125;</span><br></pre></td></tr></table></figure></p><p>其期望值为：</p><script type="math/tex; mode=display">{\displaystyle \operatorname {E} [X]=\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}</script><p>其方差为：</p><script type="math/tex; mode=display">{\displaystyle \operatorname {var} [X]=\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}</script><h3 id="正态分布-高斯分布"><a href="#正态分布-高斯分布" class="headerlink" title="正态分布(高斯分布)"></a>正态分布(高斯分布)</h3><p>若随机变量X服从一个位置参数为 ${\displaystyle \mu }$ 、尺度参数为 ${\displaystyle \sigma } $ 的概率分布，记为：</p><script type="math/tex; mode=display">X \sim N(\mu,\sigma^2),</script><p>其概率密度函数为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) = &#123;1 \over \sigma\sqrt&#123;2\pi&#125; &#125;\,e^&#123;- &#123;&#123;(x-\mu )^2 \over 2\sigma^2&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p><p>正态分布的数学期望值或期望值$ {\displaystyle \mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\displaystyle \sigma ^{2}} $ 的开平方或标准差$ {\displaystyle \sigma }$ 等于尺度参数，决定了分布的幅度。</p><h3 id="标准正态分布："><a href="#标准正态分布：" class="headerlink" title="标准正态分布："></a>标准正态分布：</h3><p>如果$ {\displaystyle \mu =0} $ 并且 $ {\displaystyle \sigma =1} $ 则这个正态分布称为标准正态分布。简化为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \, \exp\left(-\frac&#123;x^2&#125;&#123;2&#125; \right)</span><br></pre></td></tr></table></figure></p><p>如下图所示：</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png" alt="image"></p><p>正态分布中一些值得注意的量：</p><ul><li>密度函数关于平均值对称</li><li>平均值与它的众数（statistical mode）以及中位数（median）同一数值。</li><li>函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。</li><li>95.449974%的面积在平均数左右两个标准差 $ {\displaystyle 2\sigma } $ 的范围内。</li><li>99.730020%的面积在平均数左右三个标准差$ {\displaystyle 3\sigma } $ 的范围内。</li><li>99.993666%的面积在平均数左右四个标准差$ {\displaystyle 4\sigma } $ 的范围内。</li><li>函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。</li></ul><h2 id="2）多重共线性和完全共线性"><a href="#2）多重共线性和完全共线性" class="headerlink" title="2）多重共线性和完全共线性"></a>2）多重共线性和完全共线性</h2><p>多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。</p><p>完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。</p><p>两者在某种特殊情况下是有交集的。</p><h2 id="3）T检验"><a href="#3）T检验" class="headerlink" title="3）T检验"></a>3）T检验</h2><p>T检验又叫student T 检验，主要用于样本含量小，总标准差 $\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。<br>参考: <a href="http://blog.csdn.net/shulixu/article/details/53354206" target="_blank" rel="external">http://blog.csdn.net/shulixu/article/details/53354206</a></p><h2 id="4）关系"><a href="#4）关系" class="headerlink" title="4）关系"></a>4）关系</h2><ul><li>函数关系<blockquote><p>确定性关系，y=3+2x</p></blockquote></li><li>相关关系<blockquote><p>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。</p></blockquote></li></ul><h2 id="5）虚拟变量"><a href="#5）虚拟变量" class="headerlink" title="5）虚拟变量"></a>5）虚拟变量</h2><p>定义：</p><blockquote><p>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）</p></blockquote><p>作用：</p><blockquote><p>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。</p></blockquote><p>设置：</p><blockquote><p>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。<br>性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。</p></blockquote><p>例子：<br>针对上边所说的体重和身高，性别的关系。</p><p>构建模型：</p><ul><li>1）加法模型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = a + b * h + c * isman</span><br></pre></td></tr></table></figure></li></ul><p>针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。</p><ul><li>2）乘法模型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = a + b * h + c * isman * h + d * iswoman * h</span><br></pre></td></tr></table></figure></li></ul><p>同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b<em>h + c </em> h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。</p><ul><li>3）混合模型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h</span><br></pre></td></tr></table></figure></li></ul><p>假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b<em>h + c + e </em> h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。</p><h1 id="二：什么是回归（分析）"><a href="#二：什么是回归（分析）" class="headerlink" title="二：什么是回归（分析）"></a>二：什么是回归（分析）</h1><p>回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3…）和另外一组随机变量Y（Y1，Y2，Y3…）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。</p><p>回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：</p><h2 id="1）线性回归"><a href="#1）线性回归" class="headerlink" title="1）线性回归"></a>1）线性回归</h2><p>线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。</p><p>一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。</p><p>多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。</p><h2 id="2）非线性回归"><a href="#2）非线性回归" class="headerlink" title="2）非线性回归"></a>2）非线性回归</h2><p>有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。</p><h2 id="3）广义线性回归"><a href="#3）广义线性回归" class="headerlink" title="3）广义线性回归"></a>3）广义线性回归</h2><p>一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。<br>典型的代表是Logistic回归。</p><h2 id="4）如何衡量相关关系既判断适不适合使用线性回归模型？"><a href="#4）如何衡量相关关系既判断适不适合使用线性回归模型？" class="headerlink" title="4）如何衡量相关关系既判断适不适合使用线性回归模型？"></a>4）如何衡量相关关系既判断适不适合使用线性回归模型？</h2><p>使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy&gt;0,代表正相关，Rxy&lt;0,代表负相关）</p><script type="math/tex; mode=display">r_{XY} = \frac{ \sum (X_{i}-\bar{X})(Y_{i}-\bar{Y}) }{ \sqrt{ \sum (X_{i}-\bar{X})^2) \sum (Y_{i}-\bar{Y})^2) } }</script><h1 id="三：回归中困难点"><a href="#三：回归中困难点" class="headerlink" title="三：回归中困难点"></a>三：回归中困难点</h1><h2 id="1）选定变量"><a href="#1）选定变量" class="headerlink" title="1）选定变量"></a>1）选定变量</h2><blockquote><p>假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：<a href="http://blog.csdn.net/gamer_gyt/article/details/51418069" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/51418069</a> ，基本是整理《机器学习实战》这本书的笔记）</p></blockquote><h2 id="2）发现多重共线性"><a href="#2）发现多重共线性" class="headerlink" title="2）发现多重共线性"></a>2）发现多重共线性</h2><p>(1).方差扩大因子法( VIF)</p><blockquote><p>一般认为如果最大的VIF超过10，常常表示存在多重共线性。</p></blockquote><p>(2).容差容忍定法</p><blockquote><p>如果容差（tolerance）&lt;=0.1，常常表示存在多重共线性。</p></blockquote><p>(3). 条件索引</p><blockquote><p>条件索引(condition index)&gt;10，可以说明存在比较严重的共线性</p></blockquote><h2 id="3）过拟合与欠拟合问题"><a href="#3）过拟合与欠拟合问题" class="headerlink" title="3）过拟合与欠拟合问题"></a>3）过拟合与欠拟合问题</h2><p>过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。</p><p>在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。</p><p>这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。</p><h3 id="如何限制过拟合？"><a href="#如何限制过拟合？" class="headerlink" title="如何限制过拟合？"></a>如何限制过拟合？</h3><blockquote><p>过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。<br>过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。<br>当评价机器学习算法时我们有两者重要的技巧来限制过拟合<br>使用重采样来评价模型效能<br>保留一个验证数据集<br>最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。<br>验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。</p></blockquote><h2 id="4）检验模型是否合理"><a href="#4）检验模型是否合理" class="headerlink" title="4）检验模型是否合理"></a>4）检验模型是否合理</h2><p>验证目前主要采用如下三类办法：<br>1、拟合优度检验<br>主要有R^2，t检验，f检验等等<br>这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。<br>2、预测值和真实值比较<br>主要是差值和比值，一般差值和比值都不超过5%。<br>3、另外的办法<br>GEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。<br>GEH=(2(M-C)^2/(M+C))^(1/2)<br>其中M是预测值，C是实际观测值<br>如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。<br><a href="http://blog.sina.com.cn/s/blog_66188c300100hl45.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_66188c300100hl45.html</a></p><h2 id="5）线性回归的模型评判"><a href="#5）线性回归的模型评判" class="headerlink" title="5）线性回归的模型评判"></a>5）线性回归的模型评判</h2><ul><li>误差平方和（残差平方和）</li></ul><p>例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。</p><p>点到直线的距离公式为： </p><script type="math/tex; mode=display"> \frac{\left | A_{x_{0}}+B_{y_{0}} +C \right |}{\sqrt{A^2 + B^2 }}</script><p>由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：</p><script type="math/tex; mode=display">RSS = \sum_{i=1}^{n}(y_{i}- \hat{y_{i}} )^2 = \sum_{i=1}^{n}[y_{i} - (\alpha +\beta x_{i})]^2</script><ul><li>AIC准则（赤池信息准则）<script type="math/tex; mode=display">AIC=n ln (RSSp/n)+2p</script>n为变量总个数，p为选出的变量个数，AIC越小越好</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。&lt;/p&gt;
&lt;p&gt;CSDN链接：&lt;a href=&quot;http://blog.csdn.net/gamer_gyt/article/details/78008144&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击阅读&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="回归分析" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="正态分布" scheme="http://yoursite.com/tags/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/"/>
    
      <category term="T检验" scheme="http://yoursite.com/tags/T%E6%A3%80%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>数据归一化和其在sklearn中的处理</title>
    <link href="http://yoursite.com/2017/09/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96%E5%92%8C%E5%85%B6%E5%9C%A8sklearn%E4%B8%AD%E7%9A%84%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2017/09/01/机器学习/数据归一化和其在sklearn中的处理/</id>
    <published>2017-09-01T03:33:50.000Z</published>
    <updated>2017-11-12T17:06:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：数据归一化"><a href="#一：数据归一化" class="headerlink" title="一：数据归一化"></a>一：数据归一化</h1><p>数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。<br><a id="more"></a><br>归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。</p><h2 id="1）min-max标准化"><a href="#1）min-max标准化" class="headerlink" title="1）min-max标准化"></a>1）min-max标准化</h2><p>min-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下：</p><script type="math/tex; mode=display">X_{scale} = \frac{x-min}{max-min}</script><p>对应的python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class="line">def Normalization(x):</span><br><span class="line">    return [(float(i)-min(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p><p>如果要将数据转换到[-1,1]之间，可以修改其数学公式为：</p><script type="math/tex; mode=display">X_{scale} = \frac{x-x_{mean}}{max-min}</script><p>x_mean 表示平均值。</p><p>对应的python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class="line">def Normalization(x):</span><br><span class="line">    return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p><p>其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。</p><p>该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。</p><p>当然还有一些其他的办法也能实现数据的标准化。</p><h2 id="2）z-score标准化"><a href="#2）z-score标准化" class="headerlink" title="2）z-score标准化"></a>2）z-score标准化</h2><p>z-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为</p><script type="math/tex; mode=display">X_{scale} = \frac{x-\mu }{\sigma }</script><p>其中μ为所有样本数据的均值，σ为所有样本数据的标准差。</p><p>其对应的python实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">def z_score(x):</span><br><span class="line">    return (x - np.mean(x) )/np.std(x, ddof = 1)</span><br></pre></td></tr></table></figure></p><p>z-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。</p><h2 id="3）改进的z-score标准化"><a href="#3）改进的z-score标准化" class="headerlink" title="3）改进的z-score标准化"></a>3）改进的z-score标准化</h2><p>将标准分公式中的均值改为中位数，将标准差改为绝对偏差。</p><script type="math/tex; mode=display">X_{scale} = \frac{x-x_{center} }{\sigma_{1} }</script><p>中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。</p><p>σ1为所有样本数据的绝对偏差,其计算公式为：</p><script type="math/tex; mode=display">\frac{1}{N} \sum_{1}^{n}|x_{i} - x_{center}|</script><hr><h1 id="二：sklearn中的归一化"><a href="#二：sklearn中的归一化" class="headerlink" title="二：sklearn中的归一化"></a>二：sklearn中的归一化</h1><p>sklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。</p><h2 id="1）均值-标准差缩放"><a href="#1）均值-标准差缩放" class="headerlink" title="1）均值-标准差缩放"></a>1）均值-标准差缩放</h2><p>即我们上边对应的z-score标准化。<br>在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。</p><p>实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。</p><p>例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。</p><p>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import preprocessing</span><br><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class="line">...                     [ 2.,  0.,  0.],</span><br><span class="line">...                     [ 0.,  1., -1.]])</span><br><span class="line">&gt;&gt;&gt; X_scaled = preprocessing.scale(X_train)</span><br><span class="line">&gt;&gt;&gt; X_scaled</span><br><span class="line">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class="line">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class="line">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p><p>标准化后的数据符合标准正太分布<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_scaled.mean(axis=0)</span><br><span class="line">array([ 0.,  0.,  0.])</span><br><span class="line">&gt;&gt;&gt; X_scaled.std(axis=0)</span><br><span class="line">array([ 1.,  1.,  1.])</span><br></pre></td></tr></table></figure></p><p>预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; scaler = preprocessing.StandardScaler().fit(X_train)</span><br><span class="line">&gt;&gt;&gt; scaler</span><br><span class="line">StandardScaler(copy=True, with_mean=True, with_std=True)</span><br><span class="line">&gt;&gt;&gt; scaler.mean_</span><br><span class="line">array([ 1.        ,  0.        ,  0.33333333])</span><br><span class="line">&gt;&gt;&gt; scaler.scale_</span><br><span class="line">array([ 0.81649658,  0.81649658,  1.24721913])</span><br><span class="line">&gt;&gt;&gt; scaler.transform(X_train)</span><br><span class="line">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class="line">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class="line">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p><p>使用转换器可以对新数据进行转换<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_test = [[-1., 1., 0.]]</span><br><span class="line">&gt;&gt;&gt; scaler.transform(X_test)</span><br><span class="line">array([[-2.44948974,  1.22474487, -0.26726124]])</span><br></pre></td></tr></table></figure></p><h2 id="2）min-max标准化"><a href="#2）min-max标准化" class="headerlink" title="2）min-max标准化"></a>2）min-max标准化</h2><p>X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class="line">...                      [ 2.,  0.,  0.],</span><br><span class="line">...                      [ 0.,  1., -1.]])</span><br><span class="line">&gt;&gt;&gt; min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">&gt;&gt;&gt; X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class="line">&gt;&gt;&gt; X_train_minmax</span><br><span class="line">array([[ 0.5       ,  0.        ,  1.        ],</span><br><span class="line">       [ 1.        ,  0.5       ,  0.33333333],</span><br><span class="line">       [ 0.        ,  1.        ,  0.        ]])</span><br></pre></td></tr></table></figure><p>上边我们创建的min_max_scaler 同样适用于新的测试数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class="line">&gt;&gt;&gt; X_test_minmax = min_max_scaler.transform(X_test)</span><br><span class="line">&gt;&gt;&gt; X_test_minmax</span><br><span class="line">array([[-1.5       ,  0.        ,  1.66666667]])</span><br></pre></td></tr></table></figure></p><p>可以通过scale_和min方法查看标准差和最小值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; min_max_scaler.scale_ </span><br><span class="line">array([ 0.5       ,  0.5       ,  0.33333333])</span><br><span class="line">&gt;&gt;&gt; min_max_scaler.min_</span><br><span class="line">array([ 0.        ,  0.5       ,  0.33333333])</span><br></pre></td></tr></table></figure></p><h2 id="3）最大值标准化"><a href="#3）最大值标准化" class="headerlink" title="3）最大值标准化"></a>3）最大值标准化</h2><p>对于每个数值／每个维度的最大值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_train</span><br><span class="line">array([[ 1., -1.,  2.],</span><br><span class="line">       [ 2.,  0.,  0.],</span><br><span class="line">       [ 0.,  1., -1.]])</span><br><span class="line">&gt;&gt;&gt; max_abs_scaler = preprocessing.MaxAbsScaler()</span><br><span class="line">&gt;&gt;&gt; X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class="line">&gt;&gt;&gt; X_train_maxabs</span><br><span class="line">array([[ 0.5, -1. ,  1. ],</span><br><span class="line">       [ 1. ,  0. ,  0. ],</span><br><span class="line">       [ 0. ,  1. , -0.5]])</span><br><span class="line">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class="line">&gt;&gt;&gt; X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class="line">&gt;&gt;&gt; X_test_maxabs                 </span><br><span class="line">array([[-1.5, -1. ,  2. ]])</span><br><span class="line">&gt;&gt;&gt; max_abs_scaler.scale_         </span><br><span class="line">array([ 2.,  1.,  2.])</span><br></pre></td></tr></table></figure><h2 id="4）规范化"><a href="#4）规范化" class="headerlink" title="4）规范化"></a>4）规范化</h2><p>规范化是文本分类和聚类中向量空间模型的基础</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = [[ 1., -1.,  2.],</span><br><span class="line">...      [ 2.,  0.,  0.],</span><br><span class="line">...      [ 0.,  1., -1.]]</span><br><span class="line">&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm=&apos;l2&apos;)</span><br><span class="line">&gt;&gt;&gt; X_normalized</span><br><span class="line">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class="line">       [ 1.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  0.70710678, -0.70710678]])</span><br></pre></td></tr></table></figure><p>解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。</p><p>机器学习中的范数规则：<a href="http://blog.csdn.net/zouxy09/article/details/24971995/" target="_blank" rel="external">点击阅读</a><br><br>其他对应参数：<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize" target="_blank" rel="external">点击查看</a></p><p>preprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; normalizer = preprocessing.Normalizer().fit(X)</span><br><span class="line">&gt;&gt;&gt; normalizer</span><br><span class="line">Normalizer(copy=True, norm=&apos;l2&apos;)</span><br><span class="line">&gt;&gt;&gt; normalizer.transform(X)</span><br><span class="line">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class="line">       [ 1.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  0.70710678, -0.70710678]])</span><br><span class="line">&gt;&gt;&gt; normalizer.transform([[-1,1,0]])</span><br><span class="line">array([[-0.70710678,  0.70710678,  0.        ]])</span><br></pre></td></tr></table></figure></p><h2 id="5）二值化"><a href="#5）二值化" class="headerlink" title="5）二值化"></a>5）二值化</h2><p>将数据转换到0-1 之间<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X</span><br><span class="line">[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]</span><br><span class="line">&gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X)</span><br><span class="line">&gt;&gt;&gt; binarizer</span><br><span class="line">Binarizer(copy=True, threshold=0.0)</span><br><span class="line">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class="line">array([[ 1.,  0.,  1.],</span><br><span class="line">       [ 1.,  0.,  0.],</span><br><span class="line">       [ 0.,  1.,  0.]])</span><br></pre></td></tr></table></figure></p><p>可以调整二值化的门阀<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; binarizer = preprocessing.Binarizer(threshold=1.1)</span><br><span class="line">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class="line">array([[ 0.,  0.,  1.],</span><br><span class="line">       [ 1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p><h2 id="6）编码的分类特征"><a href="#6）编码的分类特征" class="headerlink" title="6）编码的分类特征"></a>6）编码的分类特征</h2><p>通常情况下，特征不是作为连续值给定的。例如一个人可以有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;male&quot;, &quot;female&quot;], [&quot;from Europe&quot;, &quot;from US&quot;, &quot;from Asia&quot;], [&quot;uses Firefox&quot;, &quot;uses Chrome&quot;, &quot;uses Safari&quot;, &quot;uses Internet Explorer&quot;]</span><br></pre></td></tr></table></figure></p><p>这些特征可以被有效的编码为整数，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&quot;male&quot;, &quot;from US&quot;, &quot;uses Internet Explorer&quot;] =&gt; [0, 1, 3]</span><br><span class="line">[&quot;female&quot;, &quot;from Asia&quot;, &quot;uses Chrome&quot;] would be [1, 2, 1].</span><br></pre></td></tr></table></figure></p><p>这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder()</span><br><span class="line">&gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])</span><br><span class="line">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class="line">       handle_unknown=&apos;error&apos;, n_values=&apos;auto&apos;, sparse=True)</span><br><span class="line">&gt;&gt;&gt; enc.transform([[0,1,3]]).toarray()</span><br><span class="line">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])</span><br></pre></td></tr></table></figure><p>默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder(n_values=[2,3,4])</span><br><span class="line">&gt;&gt;&gt; enc.fit([[1,2,3],[0,2,0]])</span><br><span class="line">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class="line">       handle_unknown=&apos;error&apos;, n_values=[2, 3, 4], sparse=True)</span><br><span class="line">&gt;&gt;&gt; enc.transform([[1,0,0]]).toarray()</span><br><span class="line">array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p><h2 id="7）填补缺失值"><a href="#7）填补缺失值" class="headerlink" title="7）填补缺失值"></a>7）填补缺失值</h2><p>由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn.preprocessing import Imputer</span><br><span class="line">&gt;&gt;&gt; imp = Imputer(missing_values=&apos;NaN&apos;,strategy=&apos;mean&apos;,verbose=0)</span><br><span class="line">&gt;&gt;&gt; imp.fit([[1, 2], [np.nan, 3], [7, 6]])</span><br><span class="line">Imputer(axis=0, copy=True, missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, verbose=0)</span><br><span class="line">&gt;&gt;&gt; X = [[np.nan, 2], [6, np.nan], [7, 6]]</span><br><span class="line">&gt;&gt;&gt; print(imp.transform(X))                           </span><br><span class="line">[[ 4.          2.        ]</span><br><span class="line"> [ 6.          3.66666667]</span><br><span class="line"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p><p>Imputer同样支持稀疏矩阵<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import scipy.sparse as sp</span><br><span class="line">&gt;&gt;&gt; X = sp.csc_matrix([[1,2],[0,3],[7,6]])</span><br><span class="line">&gt;&gt;&gt; imp = Imputer(missing_values=0,strategy=&apos;mean&apos;,axis=0)</span><br><span class="line">&gt;&gt;&gt; imp.fit(X)</span><br><span class="line">Imputer(axis=0, copy=True, missing_values=0, strategy=&apos;mean&apos;, verbose=0)</span><br><span class="line">&gt;&gt;&gt; X_test = sp.csc</span><br><span class="line">sp.csc          sp.csc_matrix(  </span><br><span class="line">&gt;&gt;&gt; X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])</span><br><span class="line">&gt;&gt;&gt; print(imp.transform(X_test))</span><br><span class="line">[[ 4.          2.        ]</span><br><span class="line"> [ 6.          3.66666667]</span><br><span class="line"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p><h2 id="8）生成多项式特征"><a href="#8）生成多项式特征" class="headerlink" title="8）生成多项式特征"></a>8）生成多项式特征</h2><p>通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。</p><p>其遵循的原则是 </p><script type="math/tex; mode=display">(X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)</span><br><span class="line">&gt;&gt;&gt; X                                                 </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [2, 3],</span><br><span class="line">       [4, 5]])</span><br><span class="line">&gt;&gt;&gt; poly = PolynomialFeatures(2)</span><br><span class="line">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class="line">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class="line">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class="line">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure><p>有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = np.arange(9).reshape(3, 3)</span><br><span class="line">&gt;&gt;&gt; X                                                 </span><br><span class="line">array([[0, 1, 2],</span><br><span class="line">       [3, 4, 5],</span><br><span class="line">       [6, 7, 8]])</span><br><span class="line">&gt;&gt;&gt; poly = PolynomialFeatures(degree=3, interaction_only=True)</span><br><span class="line">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class="line">array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],</span><br><span class="line">       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],</span><br><span class="line">       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])</span><br></pre></td></tr></table></figure></p><p>其遵循的规则是：</p><script type="math/tex; mode=display">(X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)</script><hr><p>对应的scikit-learn资料为： <a href="http://scikit-learn.org/stable/modules/preprocessing.html" target="_blank" rel="external">http://scikit-learn.org/stable/modules/preprocessing.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一：数据归一化&quot;&gt;&lt;a href=&quot;#一：数据归一化&quot; class=&quot;headerlink&quot; title=&quot;一：数据归一化&quot;&gt;&lt;/a&gt;一：数据归一化&lt;/h1&gt;&lt;p&gt;数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。&lt;br&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="数据归一化" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>MachingLearning中的距离和相似性计算以及python实现</title>
    <link href="http://yoursite.com/2017/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MachingLearning%E4%B8%AD%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E8%AE%A1%E7%AE%97%E4%BB%A5%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2017/07/16/机器学习/MachingLearning中的距离和相似性计算以及python实现/</id>
    <published>2017-07-16T04:14:43.000Z</published>
    <updated>2017-11-12T17:06:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。<br><a id="more"></a><br>在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。</p><p>文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。</p><p>参考：<a href="http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html" target="_blank" rel="external">http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html</a></p><p>推荐：<a href="https://my.oschina.net/hunglish/blog/787596" target="_blank" rel="external">https://my.oschina.net/hunglish/blog/787596</a></p><h1 id="欧氏距离"><a href="#欧氏距离" class="headerlink" title="欧氏距离"></a>欧氏距离</h1><p>也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。</p><h2 id="二维空间的欧氏距离"><a href="#二维空间的欧氏距离" class="headerlink" title="二维空间的欧氏距离"></a>二维空间的欧氏距离</h2><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离</p><script type="math/tex; mode=display">d12 =\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}</script><p>python 实现为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line">from numpy import *</span><br><span class="line"></span><br><span class="line">def twoPointDistance(a,b):</span><br><span class="line">d = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 )</span><br><span class="line">return d</span><br><span class="line"></span><br><span class="line">print &apos;a,b 二维距离为：&apos;,twoPointDistance((1,1),(2,2))</span><br></pre></td></tr></table></figure><h2 id="三维空间的欧氏距离"><a href="#三维空间的欧氏距离" class="headerlink" title="三维空间的欧氏距离"></a>三维空间的欧氏距离</h2><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离</p><script type="math/tex; mode=display">d12 =\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}</script><p>python 实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def threePointDistance(a,b):</span><br><span class="line">d = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 )</span><br><span class="line">return d</span><br><span class="line"></span><br><span class="line">print &apos;a,b 三维距离为：&apos;,threePointDistance((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p><h2 id="多维空间的欧氏距离"><a href="#多维空间的欧氏距离" class="headerlink" title="多维空间的欧氏距离"></a>多维空间的欧氏距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离</p><script type="math/tex; mode=display">\sqrt{\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }</script><p>python 实现为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def distance(a,b):</span><br><span class="line">sum = 0</span><br><span class="line">for i in range(len(a)):</span><br><span class="line">sum += (a[i]-b[i])**2</span><br><span class="line">return sqrt(sum)</span><br><span class="line"></span><br><span class="line">print &apos;a,b 多维距离为：&apos;,distance((1,1,2,2),(2,2,4,4))</span><br></pre></td></tr></table></figure><p>这里传入的参数可以是任意维的，该公式也适应上边的二维和三维</p><h1 id="标准欧氏距离"><a href="#标准欧氏距离" class="headerlink" title="标准欧氏距离"></a>标准欧氏距离</h1><p>标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：</p><p>　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：　　</p><script type="math/tex; mode=display">X^* = \frac{X-m}{s}</script><p>标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差</p><p>经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：</p><script type="math/tex; mode=display">d_{12} =\sqrt {\sum_{k=1}^{n} (\frac{x_{1k}-x_{2k}}{s_{k}})^2}</script><p>如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。</p><p>python 实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def moreBZOSdis(a,b):</span><br><span class="line">    sumnum = 0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        # 计算si 分量标准差</span><br><span class="line">        avg = (a[i]-b[i])/2</span><br><span class="line">        si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 )</span><br><span class="line">        sumnum += ((a[i]-b[i])/si ) ** 2</span><br><span class="line"></span><br><span class="line">    return sqrt(sumnum)</span><br><span class="line"></span><br><span class="line">print &apos;a,b 标准欧式距离：&apos;,moreBZOSdis((1,2,1,2),(3,3,3,4))</span><br></pre></td></tr></table></figure></p><h1 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h1><p>又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。</p><p>在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。</p><p>这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，<a href="http://download.csdn.net/detail/gamer_gyt/9899825" target="_blank" rel="external">一种人脸表情分类的新方法——Manhattan距离</a></p><h2 id="二维曼哈顿距离"><a href="#二维曼哈顿距离" class="headerlink" title="二维曼哈顿距离"></a>二维曼哈顿距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p><script type="math/tex; mode=display">d12 =\left | x_{1}-x_{2} \right |  + \left |y_{1}-y_{2}  \right |</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def twoMHDdis(a,b):</span><br><span class="line">    return abs(a[0]-b[0])+abs(a[1]-b[1])</span><br><span class="line"></span><br><span class="line">print &apos;a,b 二维曼哈顿距离为：&apos;, twoMHDdis((1,1),(2,2))</span><br></pre></td></tr></table></figure></p><h2 id="三维曼哈顿距离"><a href="#三维曼哈顿距离" class="headerlink" title="三维曼哈顿距离"></a>三维曼哈顿距离</h2><p>三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离</p><script type="math/tex; mode=display">d12 =\left | x_{1}-x_{2} \right |  + \left |y_{1}-y_{2}  \right | + \left |z_{1}-z_{2}  \right |</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def threeMHDdis(a,b):</span><br><span class="line">return abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2])</span><br><span class="line"> </span><br><span class="line">print &apos;a,b 三维曼哈顿距离为：&apos;, threeMHDdis((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p><h2 id="多维曼哈顿距离"><a href="#多维曼哈顿距离" class="headerlink" title="多维曼哈顿距离"></a>多维曼哈顿距离</h2><p>多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p><script type="math/tex; mode=display">d12 = \sum_{k=1}^{n} \left | x_{1k} - x_{2k} \right |</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def moreMHDdis(a,b):</span><br><span class="line">    sum = 0 </span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        sum += abs(a[i]-b[i])</span><br><span class="line">    return sum</span><br><span class="line"></span><br><span class="line">print &apos;a,b 多维曼哈顿距离为：&apos;, moreMHDdis((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p><p>由于维距离计算是比较灵活的，所以也同样适合二维和三维。</p><h1 id="切比雪夫距离"><a href="#切比雪夫距离" class="headerlink" title="切比雪夫距离"></a>切比雪夫距离</h1><p>切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , … ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中<a href="http://blog.csdn.net/jerry81333/article/details/52632687" target="_blank" rel="external">曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别</a> 给了一个很形象的解释如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。</span><br><span class="line"></span><br><span class="line">但是按照切比雪夫距离，这是完全不同的概念了。</span><br><span class="line"></span><br><span class="line">譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。</span><br><span class="line"></span><br><span class="line">这样来看的话，距离是不是就不一样了呢？</span><br><span class="line"></span><br><span class="line">或者还是不清楚，我再说的详细点。</span><br><span class="line"></span><br><span class="line">同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。</span><br></pre></td></tr></table></figure></p><h2 id="二维切比雪夫距离"><a href="#二维切比雪夫距离" class="headerlink" title="二维切比雪夫距离"></a>二维切比雪夫距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离</p><script type="math/tex; mode=display">d_{12} = max( \left | x_{1} - x_{2} \right | , \left | y_{1} - y_{2} \right |)</script><p>python 实现为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def twoQBXFdis(a,b):</span><br><span class="line">    return max( abs(a[0]-b[0]), abs(a[1]-b[1]))</span><br><span class="line"></span><br><span class="line">print &apos;a,b二维切比雪夫距离：&apos; , twoQBXFdis((1,2),(3,4))</span><br></pre></td></tr></table></figure><h2 id="多维切比雪夫距离"><a href="#多维切比雪夫距离" class="headerlink" title="多维切比雪夫距离"></a>多维切比雪夫距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离</p><script type="math/tex; mode=display">d12 = max_{i\epsilon n}( \left | x_{1i} - x_{2i} \right | )</script><p>python 实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def moreQBXFdis(a,b):</span><br><span class="line">    maxnum = 0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        if abs(a[i]-b[i]) &gt; maxnum:</span><br><span class="line">            maxnum = abs(a[i]-b[i])</span><br><span class="line">    return maxnum</span><br><span class="line"></span><br><span class="line">print &apos;a,b多维切比雪夫距离：&apos; , moreQBXFdis((1,1,1,1),(3,4,3,4))</span><br></pre></td></tr></table></figure></p><h1 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h1><p>有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为</p><script type="math/tex; mode=display">D(x) = \sqrt{(X-\mu )^TS^{-1}(X-\mu)}</script><p>而其中向量Xi与Xj之间的马氏距离定义为</p><script type="math/tex; mode=display">D(X_{i},X_{j}) = \sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )}</script><p> 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：</p><script type="math/tex; mode=display">D(X_{i},X_{j}) = \sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}</script><p>也就是欧氏距离了。</p><p>若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。</p><p>马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。</p><h1 id="夹角余弦"><a href="#夹角余弦" class="headerlink" title="夹角余弦"></a>夹角余弦</h1><p>几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。</p><h2 id="二维空间向量的夹角余弦相似度"><a href="#二维空间向量的夹角余弦相似度" class="headerlink" title="二维空间向量的夹角余弦相似度"></a>二维空间向量的夹角余弦相似度</h2><p>在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：</p><script type="math/tex; mode=display">\cos \theta  = \frac{x_{1}x_{2} + y_{1}y_{2}}{ \sqrt{ x_{1}^2+x_{2}^2 }\sqrt{ y_{1}^2+y_{2}^2 } }</script><p>python 实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def twoCos(a,b):</span><br><span class="line">    cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) )</span><br><span class="line"></span><br><span class="line">    return cos</span><br><span class="line">print &apos;a,b 二维夹角余弦距离：&apos;,twoCos((1,1),(2,2))</span><br></pre></td></tr></table></figure></p><h2 id="多维空间向量的夹角余弦相似度"><a href="#多维空间向量的夹角余弦相似度" class="headerlink" title="多维空间向量的夹角余弦相似度"></a>多维空间向量的夹角余弦相似度</h2><p>两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦</p><p>类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。</p><script type="math/tex; mode=display">\cos \theta  = \frac{a \cdot  b}{\left | a \right | \left | b \right |}</script><p>即：</p><script type="math/tex; mode=display">\cos \theta  = \frac{ \sum_{k=1}^{n} x_{1k}x_{2k} }{ \sqrt{ \sum_{k=1}^{n}x_{1k}^2 }\sqrt{ \sum_{k=1}^{n} x_{2k}^2 } }</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def moreCos(a,b):</span><br><span class="line">    sum_fenzi = 0.0</span><br><span class="line">    sum_fenmu_1,sum_fenmu_2 = 0,0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        sum_fenzi += a[i]*b[i]</span><br><span class="line">        sum_fenmu_1 += a[i]**2 </span><br><span class="line">        sum_fenmu_2 += b[i]**2 </span><br><span class="line"></span><br><span class="line">    return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )</span><br><span class="line">print &apos;a,b 多维夹角余弦距离：&apos;,moreCos((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p><p>夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。</p><h1 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h1><p>闵氏距离不是一种距离，而是一组距离的定义</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p><script type="math/tex; mode=display">\sqrt[p]{ \sum_{k=1}^{n} \left | x_{1k}-x_{2k} \right |^p}</script><p>其中p是一个变参数。</p><p>当p=1时，就是曼哈顿距离</p><p>当p=2时，就是欧氏距离</p><p>当p→∞时，就是切比雪夫距离</p><p>根据变参数的不同，闵氏距离可以表示一类的距离。</p><h2 id="闵氏距离的缺点"><a href="#闵氏距离的缺点" class="headerlink" title="闵氏距离的缺点"></a>闵氏距离的缺点</h2><p>闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。</p><p>举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。</p><p>简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。</p><h1 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。</p><p>应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。</p><h2 id="python-实现"><a href="#python-实现" class="headerlink" title="python 实现"></a>python 实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def hanmingDis(a,b):</span><br><span class="line">    sumnum = 0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        if a[i]!=b[i]:</span><br><span class="line">            sumnum += 1</span><br><span class="line">    return sumnum</span><br><span class="line"></span><br><span class="line">print &apos;a,b 汉明距离：&apos;,hanmingDis((1,1,2,3),(2,2,1,3))</span><br></pre></td></tr></table></figure><h1 id="杰卡德距离-amp-杰卡德相似系数"><a href="#杰卡德距离-amp-杰卡德相似系数" class="headerlink" title="杰卡德距离 &amp; 杰卡德相似系数"></a>杰卡德距离 &amp; 杰卡德相似系数</h1><h2 id="杰卡德距离"><a href="#杰卡德距离" class="headerlink" title="杰卡德距离"></a>杰卡德距离</h2><p>与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：</p><script type="math/tex; mode=display">J_{\delta} (A,B) = \frac{| A \bigcup B | - | A \bigcap B |}{| A \bigcup B |}</script><p>杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。</p><p>python 实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def jiekadeDis(a,b):</span><br><span class="line">    set_a = set(a)</span><br><span class="line">    set_b = set(b)</span><br><span class="line">    dis = float(len( (set_a | set_b) - (set_a &amp; set_b) ) )/ len(set_a | set_b)</span><br><span class="line">    return dis</span><br><span class="line"></span><br><span class="line">print &apos;a,b 杰卡德距离：&apos;, jiekadeDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p><h2 id="杰卡德相似系数"><a href="#杰卡德相似系数" class="headerlink" title="杰卡德相似系数"></a>杰卡德相似系数</h2><p>两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。</p><script type="math/tex; mode=display">J(A,B) = \frac{| A \bigcap B |}{| A \bigcup B |}</script><p>杰卡德相似系数是衡量两个集合的相似度一种指标。</p><p>python 实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def jiekadeXSDis(a,b):</span><br><span class="line">    set_a = set(a)</span><br><span class="line">    set_b = set(b)</span><br><span class="line">    dis = float(len(set_a &amp; set_b)  )/ len(set_a | set_b)</span><br><span class="line">    return dis</span><br><span class="line"></span><br><span class="line">print &apos;a,b 杰卡德相似系数：&apos;, jiekadeXSDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p><h2 id="杰卡德相似系数与杰卡德距离的应用"><a href="#杰卡德相似系数与杰卡德距离的应用" class="headerlink" title="杰卡德相似系数与杰卡德距离的应用"></a>杰卡德相似系数与杰卡德距离的应用</h2><p>可将杰卡德相似系数用在衡量样本的相似度上。</p><p>　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。</p><p>p ：样本A与B都是1的维度的个数</p><p>q ：样本A是1，样本B是0的维度的个数</p><p>r ：样本A是0，样本B是1的维度的个数</p><p>s ：样本A与B都是0的维度的个数</p><p>那么样本A与B的杰卡德相似系数可以表示为：</p><p>这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。</p><p>而样本A与B的杰卡德距离表示为：</p><script type="math/tex; mode=display">J= \frac{p}{p+q+r}</script><h1 id="相关系数-amp-相关距离"><a href="#相关系数-amp-相关距离" class="headerlink" title="相关系数 &amp; 相关距离"></a>相关系数 &amp; 相关距离</h1><h2 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h2><script type="math/tex; mode=display">\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}}=\frac{ E( (X-EX) (Y-EY) ) }{ \sqrt{D(X)} \sqrt{D(Y)} }</script><p>相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。</p><p>python 实现<br>相关系数可以利用numpy库中的corrcoef函数来计算<br>例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from numpy import  *</span><br><span class="line">a = array([[1, 1, 2, 2, 3],  </span><br><span class="line">       [2, 2, 3, 3, 5],  </span><br><span class="line">       [1, 4, 2, 2, 3]]) </span><br><span class="line"></span><br><span class="line">print corrcoef(a)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 1.        ,  0.97590007,  0.10482848],</span><br><span class="line">       [ 0.97590007,  1.        ,  0.17902872],</span><br><span class="line">       [ 0.10482848,  0.17902872,  1.        ]])</span><br><span class="line"></span><br><span class="line">print corrcoef(a,rowvar=0)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class="line">       [-0.18898224,  1.        , -0.18898224, -0.18898224, -0.18898224],</span><br><span class="line">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class="line">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class="line">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ]])</span><br></pre></td></tr></table></figure></p><h2 id="相关距离"><a href="#相关距离" class="headerlink" title="相关距离"></a>相关距离</h2><script type="math/tex; mode=display">D_{xy} = 1 - \rho _{XY}</script><p>python 实现（基于相关系数）<br>同样针对矩阵a<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 行之间的相关距离</span><br><span class="line">ones(shape(corrcoef(a)),int) - corrcoef(a)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 0.        ,  0.02409993,  0.89517152],</span><br><span class="line">       [ 0.02409993,  0.        ,  0.82097128],</span><br><span class="line">       [ 0.89517152,  0.82097128,  0.        ]])</span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line"># 列之间的相关距离</span><br><span class="line">ones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 1.18898224,  0.        ,  1.18898224,  1.18898224,  1.18898224],</span><br><span class="line">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ]])</span><br></pre></td></tr></table></figure></p><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。</p><p>计算给定的样本集X的信息熵的公式：</p><script type="math/tex; mode=display">Entropy(X) = \sum_{i=1}^{n} -p_{i} log_{2}p_{i}</script><p>参数的含义：</p><p>n：样本集X的分类数</p><p>pi：X中第i类元素出现的概率</p><p>信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0</p><p>python进行计算和实现可参考：<br><a href="http://blog.csdn.net/autoliuweijie/article/details/52244246" target="_blank" rel="external">http://blog.csdn.net/autoliuweijie/article/details/52244246</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。&lt;br&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="相似度计算" scheme="http://yoursite.com/tags/%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦</title>
    <link href="http://yoursite.com/2017/04/16/%E9%9A%8F%E6%89%8B%E8%AE%B0/%E4%B8%80%E5%88%87%E7%9A%84%E9%97%B9%E9%97%B9%E5%93%84%E5%93%84%EF%BC%8C%E5%8F%AA%E6%98%AF%E4%BB%96%E5%9C%A8%E6%B0%B4%E5%B8%98%E6%B4%9E%E8%BA%B2%E9%81%BF%E9%A3%8E%E6%B2%99%E9%82%A3%E6%99%9A%E5%81%9A%E7%9A%84%E4%B8%80%E4%B8%AA%E6%A2%A6/"/>
    <id>http://yoursite.com/2017/04/16/随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦/</id>
    <published>2017-04-15T17:03:56.000Z</published>
    <updated>2017-11-12T17:08:19.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。</p></blockquote><a id="more"></a><p><img src="http://img.blog.csdn.net/20170415205051154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦。"></p><hr><h2 id="对《大话》的解读"><a href="#对《大话》的解读" class="headerlink" title="对《大话》的解读"></a><strong>对《大话》的解读</strong></h2><p>有人说这是一部烂片，也有人说这是一部经典，有人说这是对西游文化的过度消费，有人说这是一代人的反思与成长，有人说阴差阳错的蝴蝶效应，让一个年代的人支撑起了这部超越中国电影水平50年的无厘头式作品，也有人说这是一部笑中带泪，总有一天你走在路上也会像一条狗的悲剧。</p><p>一千个人眼中有一千个哈姆雷特，对于一部影视作品，我们无法去评论任何一个人的理解是对还是错，因为经历不同，就像你不能把你的情感强加到别人身上。喜欢一部作品，在不知不觉间你会忽略他的缺点，讨厌一个事物，在不知不觉间就会放大他的短处。我们也不能用我们现代的思维和观念去评判20世纪港台电影的文化和情感。</p><p>至尊宝的路线是，有人给他点了三颗痣，拔出来紫青宝剑，开启了与紫霞的交涉，娶亲，戴上紧箍咒，大战牛魔王，西天取经。其实至尊宝这条路线，何尝不是我们的漫漫人生路呢，至尊宝用月光宝盒来寻找500年前的白晶晶，却阴差阳错遇到了紫霞仙子，最后在对白晶晶失而复得之后又得知紫霞在他内心留下的一滴眼泪，最终大彻大悟，明白了舍生取义，参透了生亦何哀，死亦何苦。我们的人生不也一样，总会有各种各样的未知因素影响我们的选择，在努力面对眼前一切之后，最终回头感叹年华，围炉小饮，本来无一物，何处惹尘埃。</p><p>对于紫霞和至尊宝的爱情，没有这种体验，是不会懂的，越是深情的人越容易受伤，她放下尊严来爱他，终于他后悔了。在至尊宝的梦里，紫霞仙子说：“飞蛾明明知道前面是火堆，却还义无反顾的扑进去。”她笑一下，接着说：“飞蛾就是这么傻！”而同样的至尊宝，“有一个名字叫紫霞的，你叫七百八十四次“，时光真是个好东西，所谓雕刻时光，是说生活像把钝刀，锉平我们的触觉，而电影则是解毒的重药，它让人突然领悟到，我们的内心要比自己想像的敏感干净得多！你等的那么辛苦，他却陌生到让你心疼。以至于在人生的不同阶段去欣赏《大话》，似乎都能从中找到某些影子，原先是笑，后来是苦，到最后便是沉默了。城墙上的一吻，至尊宝变成了旁观者，他借用夕阳武士的身份和转世的紫霞完成了他告别的深情，于是他的背影，他的离开，寂寞成了“好像一条狗”。</p><hr><h2 id="他好像一条狗呀！"><a href="#他好像一条狗呀！" class="headerlink" title="他好像一条狗呀！"></a><strong>他好像一条狗呀！</strong></h2><p>-“那个人的样子好怪啊”<br>-“我也看到了，他好像条狗啊”<br>是呀，英雄的离开，留下的永远只是背影，只不过在至尊宝这里加上了悲剧色彩，加上了人艰不拆的辛酸泪，以至于多少年后才明白了至尊宝转身离开有多难。至尊宝的结局是一个男人的悲怆与无奈。</p><p>至尊宝用了月光宝盒来寻找500年前的白晶晶，同时遇到了在她看来，他是她的命中注定的紫霞仙子，直到后来牛魔王的出现，夺走了紫霞，夺走了白晶晶，夺走了至尊宝往日的快乐，他明白他要夺回这一切，可是面对戏剧般的月光宝盒，至尊宝得到的更多是无力和苍白，面对这些无情的现实，幻想一次又一次地破灭。直到最后的关头，至尊宝终于醒悟，靠月光宝盒不行，至尊宝更是没有那个本事，只有成为孙悟空，只有戴上那个金刚圈，他才有能力同牛魔王一较高下。</p><p>这是一个极大的讽刺。你想要得到吗？好，那么你必须先放弃至尊宝的身份，你必须做出选择，必须忍受无尽的痛苦，他想要化解时间无尽的仇恨，就必须放弃自己的感情，不是不爱，而是大彻大悟之后的大爱，他必需化身为孙悟空，帮助唐三藏取得经书，化解这世间的恨。</p><p>那么至尊宝的放弃是自觉自愿的醒悟吗？不，他并不愿意，但是他必须拯救紫霞，必须化解人间的恨，他别无选择，他必须戴上紧箍咒。虽然成为了孙悟空，成了大英雄，但他对自己的生存状态极度不满。所以在最后，孙悟空将他心中残存的至尊宝的影子幻化作一位夕阳武士，在对现实世界彻底失望后，只能构造一个虚幻的想象来了却这桩心愿，并借武士的口中表达了对自己生存状态的不满，活得好象是一条狗一样。唉，一个男人的悲怆和无奈。</p><hr><h2 id="那句意中人，满足了多少人的少女心"><a href="#那句意中人，满足了多少人的少女心" class="headerlink" title="那句意中人，满足了多少人的少女心"></a><strong>那句意中人，满足了多少人的少女心</strong></h2><p>“我知道有一天，他会在一个万众瞩目的情况下出现，身披金甲圣衣，脚踏七色云彩来娶我”<br>“我的意中人是个盖世英雄，有一天他会踏着云彩来娶我”<br>这两句分别是紫霞在牛魔王娶她前的晚上和死前对孙悟空说的，多么经典的台词，以至于现在多少人还幻想着自己的意中人。</p><p>进入至尊宝内心的只有两个女人，一个是白晶晶，一个是紫霞，白晶晶问的是“他最喜欢的人是不是我”，紫霞问的是“他跟他的娘子是不是很恩爱呀？”，白晶晶的爱是一种索求的爱，而紫霞的爱则是无怨无悔的。</p><p>所以最终至尊宝回来了，在化身为孙悟空之后，身披金甲圣衣，脚踏七色云彩而来，他实现了紫霞的梦想，只不过加了一层掩饰与牵强。</p><p>从现如今这个角度反思紫霞的意中人，我是不太赞同的，童话毕竟是童话，正是这个经典的对白，让多少人活在自己的想象中，我们都渴望对方是个“意中人”的形象，可是我们却忽略乐一个”等价“的观念，你凭什么拥有你的”意中人“，你配得上吗？这不仅让我想起了另外一个命题：“不要去羡慕那些散发光芒的成功者，因为你不知道他背后付出的努力和艰辛”，这其实是一个道理，如果你仅仅是停留在幻想和计划的层面，那么你永远得不到你的”意中人“。</p><hr><h2 id="我猜中了开头，可是我猜不着这结局"><a href="#我猜中了开头，可是我猜不着这结局" class="headerlink" title="我猜中了开头，可是我猜不着这结局"></a><strong>我猜中了开头，可是我猜不着这结局</strong></h2><p>紫霞说猜得到开始，却猜不透这结局。大约直到最后，她也没能明白、没能理解至尊宝的苦心。又或者说，是至尊宝从来也未能真正了解她的心意。我曾经以为，死去的紫霞是最可怜的角色。可是，至尊宝又何尝不可怜呢，他甚至，连伤心的权利也没有了。在紫霞死去的一瞬间，他的心也已经跟着死去了。在他余下的人生里，再也不会有欢笑、快乐，再也不会有那样一个可以在他心里流下眼泪的女孩子。就算取回西经又能如何，心爱的人再也会不来了。就算成佛又能如何，没有了你，整个世界对我来说都毫无意义。</p><hr><h2 id="《大话》把遗憾和难题抛给了时间"><a href="#《大话》把遗憾和难题抛给了时间" class="headerlink" title="《大话》把遗憾和难题抛给了时间"></a><strong>《大话》把遗憾和难题抛给了时间</strong></h2><p>又一次的时空穿梭后，面对城头男女，孙悟空附身夕阳武士，给出无数人热泪纵横，内心中期盼的最后答案。<br>没有失去过，也永远不能明白得到的快乐。<br>附身后的孙悟空发自内心肺腑地给了女子一记深深长吻，这一吻穿越地老天荒，不再相信自欺欺人的一万年，他那般语气坚决地说出了那三个字。<br>先前拒不让步的夕阳武士，拥抱着爱人幸福陶醉。<br>转身远去的孙悟空了却尘缘心事，消失在大漠黄沙尽头。<br>只是每次在紫霞被刺中或者孙悟空松手的瞬间，还是会心潮如水甚至潸然泪下。 　<br>十年大话，一群人围坐着观看《大话西游》的狂热时代过去了，心底保存的泪水也慢慢尘封直至故事终结。毫无拘束的开怀大笑渐渐沦为一个人的狂欢，难加掩饰的心底苍凉逐渐成了人生重担</p><hr><h2 id="如今的你，何去何从"><a href="#如今的你，何去何从" class="headerlink" title="如今的你，何去何从"></a><strong>如今的你，何去何从</strong></h2><p>“如今的你，何去何从？”<br>“对呀，何去何从”</p><p>真的羡慕至尊宝最初为了营救白晶晶，借用外力，使用月光宝盒穿越回500年前，为了解救紫霞，戴上紧箍咒。</p><p>而你呢？没有目标，你便是一个游荡的灵魂。</p><hr><h2 id="加长版加了什么"><a href="#加长版加了什么" class="headerlink" title="加长版加了什么"></a><strong>加长版加了什么</strong></h2><p>1：紫霞刚出现时在沙漠和雪蛤精，孔雀王的对话以及 他们的拔剑抢婚，与影片中紫霞和至尊宝在 牛魔王婚礼上遇见时雪蛤精，孔雀王和反对结婚作了呼应。</p><p>2：约好二更相见，原版是牛夫人出现 ，然后是至尊宝直接被猪八戒和沙师弟拉去救师父，加长版中先是牛夫人出现，然后牛魔王，然后至尊宝</p><p>3：牛魔王婚礼时猪八戒和沙僧在小妖堆里跟他们“打成一片”</p><p>4：至尊宝被青霞揍晕第二次之后，早上跟紫霞说的那通话“你要让我拿点信物给他看, 你有什么项链啊,首饰啊,金银珠宝啊,月光宝盒啊什么的……”原版的这段声音不是石班瑜所配。新版中，这段声音是重新配音，换上了石班瑜的声音。</p><p>5：2K画面的修复</p><hr><p></p><h2>一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦<p></p></h2>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="随手记" scheme="http://yoursite.com/categories/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
    
      <category term="周星驰" scheme="http://yoursite.com/tags/%E5%91%A8%E6%98%9F%E9%A9%B0/"/>
    
      <category term="大话西游" scheme="http://yoursite.com/tags/%E5%A4%A7%E8%AF%9D%E8%A5%BF%E6%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>别了青春与流年，遇见下一个自己</title>
    <link href="http://yoursite.com/2016/12/21/%E9%9A%8F%E6%89%8B%E8%AE%B0/%E5%88%AB%E4%BA%86%E9%9D%92%E6%98%A5%E4%B8%8E%E6%B5%81%E5%B9%B4%EF%BC%8C%E9%81%87%E8%A7%81%E4%B8%8B%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1/"/>
    <id>http://yoursite.com/2016/12/21/随手记/别了青春与流年，遇见下一个自己/</id>
    <published>2016-12-20T16:35:00.000Z</published>
    <updated>2017-11-11T10:05:00.000Z</updated>
    
    <content type="html"><![CDATA[<font size="3">如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。</font><a id="more"></a><h2 id="2016-时间是长了脚的妖怪，跑的飞快"><a href="#2016-时间是长了脚的妖怪，跑的飞快" class="headerlink" title="2016-时间是长了脚的妖怪，跑的飞快"></a><strong>2016-时间是长了脚的妖怪，跑的飞快</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;四年时光，匆匆而过，沈阳占据了我23岁之前的太多第一次，第一次一个人一包行李，第一次21个小时的硬座，第一次坐地铁，第一次谈恋爱，第一次分手，第一次旅行，第一次坐摩天轮，第一次吃棉花糖，第一次看电影，第一次接触电脑，第一次……<br>&nbsp;&nbsp;&nbsp;&nbsp;时间是长了脚的妖怪，跑的飞快，只是好像后来我们都离开，各自生活在喧嚣未来，当时的遗憾在回忆肆虐的某些时段，重新打开，又好象我们同时都在。<br>&nbsp;&nbsp;&nbsp;&nbsp;有人说，谈过恋爱，分过手，挂过科，拿过奖学金，当过学生干部的大学才是完美的，那么我想我还是比较幸运的，回顾我的大学生活，除了两件我极力想做的事情没有完成之外，经历了太多，谈过恋爱，分过手，当过学生干部，拿过奖学金，参加过各种志愿活动，也做过校园代理，被坑过，被骗过，也干过兼职，做过外包，送过外卖，发过传单，在这整个过程中，认识了不少人，见过不少事，看明白了不少的社会道理，看清了多少人的虚情假意，感谢那一路让我经历成长的人。<br>&nbsp;&nbsp;&nbsp;&nbsp;大一是我大学生活里最快乐的一年，那时的我们很单纯。只是后来，大家都变了。</p><p><img src="http://img.blog.csdn.net/20161220223409601" alt="这里写图片描述"></p><hr><h2 id="2016-剑未配好，已出江湖，来一场说走就走的北漂"><a href="#2016-剑未配好，已出江湖，来一场说走就走的北漂" class="headerlink" title="2016-剑未配好，已出江湖，来一场说走就走的北漂"></a><strong>2016-剑未配好，已出江湖，来一场说走就走的北漂</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;该到来的还是会到来，虽然对于工作我是做好了准备，但是还是有点措手不及。</p><ul><li><p>七月，别了流年<br>&nbsp;&nbsp;&nbsp;&nbsp;那是七月，我的心情迫切的像火辣辣的太阳，拉着行李，从大学的门前离开，没有回头，虽然这里有我牵挂的人，有我念着的事，但我还是把更多的希望寄托在充满魔性的首都，因为我相信这里是梦会是开始的地方，于是在朋友的帮助下，我开启了我的北漂生活。</p></li><li><p>广联达<br>&nbsp;&nbsp;&nbsp;&nbsp;我来北京的第一家公司是广联达，建筑行业国内算是龙头老大了，虽然在互联网行业不是太牛逼，但对于一个初出茅庐的我还是够我学习和经历了，而且凑巧的是公司是我一个八几年的校友创立的，只是这和我没有半毛钱关系，在那的三个月里，我连个人影都没有见过。<br>&nbsp;&nbsp;&nbsp;&nbsp;后来的后来我选择了离开，不是公司不好，不是带我的师父不优秀，不是同事不牛逼，只是我感觉那里不适合我。<br>&nbsp;&nbsp;&nbsp;&nbsp;我的师父是项目组组长吧，人有点娘娘腔，别人都叫他梅梅，但是对我们特别好，他技术也十分厉害，离开的时候和师父聊天，他说在公司是P3和P4的技术双认证，是一个技术架构师，自己带着团队几个月为公司写了一个云测试平台，现在更到3.x版本了吧。我个人是十分佩服我师父的，为人低调，技术够强，还有好人缘。<br>&nbsp;&nbsp;&nbsp;&nbsp;在公司的那段时候里，我主要做的是一个以课题形式展示的数据分析平台，用到的技术无非就是大学里学的那些，那个时候和另外一个同事还吹牛逼说咱也是架构师了，这仅仅是因为自己画了个水的一逼的图<br><img src="http://img.blog.csdn.net/20161220230511202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>&nbsp;&nbsp;&nbsp;&nbsp;哈哈，如果这幅图出自架构师之手，就是系统架构了，可是出自我们这等毛小子之手就是闹着玩了吧。就好比以国家的名义去挖墓，就是考古，以个人名义去挖墓就是盗墓了。</p></li><li><p>昌平线，煎熬<br>&nbsp;&nbsp;&nbsp;&nbsp;西二旗是中国最堵得一个地铁站了，大家都说后村厂路堵车十分钟，中国互联网经济停滞2小时。<br>&nbsp;&nbsp;&nbsp;&nbsp;昌平线是北漂人的聚集地了，不是因为别的，是因为这条线路上的房租便宜 ，那个时候我就盘踞在沙河高教园旁边的东沙屯村里，每月800元的房租还是负担的起的，除了交通不便之外，一切还都可以接受，毕竟你是在北漂。</p></li></ul><hr><h2 id="2016-别了流年，是现在的我"><a href="#2016-别了流年，是现在的我" class="headerlink" title="2016-别了流年，是现在的我"></a><strong>2016-别了流年，是现在的我</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;九月末我面试了现在所在的公司，离开了广联达，不是因为它不优秀，它不好，只是因为那里现在还不适合我，在我的棱角被磨平之前，我想出去闯一闯。<br>&nbsp;&nbsp;&nbsp;&nbsp;可能是我所在部门的原因，我觉得特别懒散，感觉大家都是在混日子，每天改那么点bug，每天更新一点小功能，或者这就是大公司的尴尬，或者说转型之中的公司的短板吧，大家都沉浸在以前的辉煌之中，没有创造力，没有新奇的想法，没有交流的冲动，没有那种干劲。于是我选择了离开，我想先让我去经历一番我想要的工作与生活，等我累了，说不定我就会想念这种状态了。<br>&nbsp;&nbsp;&nbsp;&nbsp;现在所在的是一个创业公司，像我想象中一样，大家窝在一个不大的办公司，交流与合作，为了梦想一起努力着，很开心。<br>&nbsp;&nbsp;&nbsp;&nbsp;在这里我接触到了更多知识，技术的，做人的，交流的，至今我脑海中还清晰的记着那天赵总的一句话：读书要有收获，至少要涨气场。<br>&nbsp;&nbsp;&nbsp;&nbsp;新的环境里我接触学习了Docker，ELK，重新学习了一些机器学习的算法知识。于是在我的CSDN博客中创建了两个技术专栏，由于刚刚接触，写的也不够深入，不过我会努力的。<br>&nbsp;&nbsp;&nbsp;&nbsp;Docker江湖：<a href="http://blog.csdn.net/column/details/13159.html" target="_blank" rel="external">http://blog.csdn.net/column/details/13159.html</a><br> &nbsp;&nbsp;&nbsp;&nbsp;ELK从入门到放弃：<a href="http://blog.csdn.net/column/details/13079.html" target="_blank" rel="external">http://blog.csdn.net/column/details/13079.html</a></p><p>&nbsp;&nbsp;&nbsp;&nbsp;认认真真经历才能好好成长。</p><hr><h2 id="2016-我在CSDN的收获"><a href="#2016-我在CSDN的收获" class="headerlink" title="2016-我在CSDN的收获"></a><strong>2016-我在CSDN的收获</strong></h2><ul><li>鲍大神<br>&nbsp;&nbsp;&nbsp;&nbsp;开始在CSDN上写博客是大一的时候，是一个牛逼的学长带我走上了这条”不归路”，谢谢<a href="http://blog.csdn.net/baolibin528" target="_blank" rel="external">鲍大神</a>这一路的指导与传授，一直以来，他都是我的榜样。我也努力赶上他，只可惜看到的永远都是背影。</li></ul><ul><li><p>梦姐姐<br>&nbsp;&nbsp;&nbsp;&nbsp;八月份的时候偶然的机会认识梦姐姐，做了博乐，后来也申请并通过了CSDN博客专家。</p></li><li><p>结识技术爱好者<br>&nbsp;&nbsp;&nbsp;&nbsp;其实相比这些更重要的是通过CSDN所认识的每一个技术爱好者，可以说CSDN是国内的程序员的社交平台了。感觉那些给我留言提问我的人，可能有些疑问还是没有帮你们解决，只是我个人能力有限，不像郭神，鸿洋大神技术功底深厚。在这个平台之上，我也认识到了自己的许多不足和技术缺点，在阅读博客的过程中，也学到了不少东西。</p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;谢谢你一路陪我成长，你若不离，我定不弃。</p><hr><h2 id="2016-开始commit我的github"><a href="#2016-开始commit我的github" class="headerlink" title="2016-开始commit我的github"></a><strong>2016-开始commit我的github</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;有人说开源垃圾，有人说开源缩减了开发的成本和时间，不管怎样，开源是一种趋势，而且势头不会减弱，很荣幸我也投入了开源的大军，即使现在我还是一个蝼蚁。<br>&nbsp;&nbsp;&nbsp;&nbsp;我的github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br><img src="http://img.blog.csdn.net/20161221000236962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><hr><h2 id="2016-杂乱无章"><a href="#2016-杂乱无章" class="headerlink" title="2016-杂乱无章"></a><strong>2016-杂乱无章</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这一年，从一个初出茅庐的蝼蚁一步步成长，一个个经历，我给交了一份70分的答卷，我没有让我的父母和亲人失望，我没有让我的老师失望，我没有让我喜欢的人失望，我也没有让曾经看不起我的人失望，只是我让自己失望了。<br>&nbsp;&nbsp;&nbsp;&nbsp;有些东西我没有去争取，有些机会我没有把握，有些冲动我失了控。但正是这些完美的不完美的，才让你有更大的劲头去前进。</p><hr><h2 id="2017-下一个自己"><a href="#2017-下一个自己" class="headerlink" title="2017-下一个自己"></a><strong>2017-下一个自己</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;时间不会因为你的遗憾而停留，我们能做的就是把每一天都当成最后一天来过。<br>&nbsp;&nbsp;&nbsp;&nbsp;2017，我要完成：</p><ul><li>一个安卓APP和对应的Web </li><li>小说《这夏未眠》</li><li>发表社区划分论文</li><li>深入学习Scala和Spark</li><li>掌握一个深度学习框架（eg：Caffe）</li><li>跟进研究Hadoop家族的最近版本，并形成文档</li><li>换一台Mackbook Pro</li><li>攒够100K+</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;感谢这一路有你，加油！</p><hr><p>个人微信公众号，欢迎关注<br><img src="http://img.blog.csdn.net/20161221002809051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="个人微信公众号，欢迎关注"></p>]]></content>
    
    <summary type="html">
    
      &lt;font size=&quot;3&quot;&gt;如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。&lt;/font&gt;
    
    </summary>
    
      <category term="随手记" scheme="http://yoursite.com/categories/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
    
      <category term="青春" scheme="http://yoursite.com/tags/%E9%9D%92%E6%98%A5/"/>
    
      <category term="总结" scheme="http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>这夏未眠.简介</title>
    <link href="http://yoursite.com/2016/09/15/%E5%A4%8F%E6%9C%AA%E7%9C%A0/%E8%BF%99%E5%A4%8F%E6%9C%AA%E7%9C%A0-%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2016/09/15/夏未眠/这夏未眠-简介/</id>
    <published>2016-09-15T10:13:50.000Z</published>
    <updated>2017-11-11T10:14:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。<br><a id="more"></a><br>两人同在陌乘一中念初中，同班同学，在中考来临的那段日子，两人相互鼓励，于是顾艾哲（小艾）考上了他从来都没有想过能考上的孟川一高，而莫晨（晨晨）呢，考上了预料之中的平阳一高，而她在这之前却从来没告诉过顾艾哲（小艾）她要去平阳，就这样，两个人分开了，一些都看起来那么顺理成章，一切又看起来那么暗淡失望。</p><p>在经历过高中的二年之后，顾艾哲（小艾）终于联系上了莫晨（晨晨），那天晚上，他用妈妈的电话给莫晨（晨晨）通了两个小时的电话，似乎要把两人两年里没有说的话都说完，可是有太多的话是无法用言语表达的，就这样电话欠费了，终止了聊天，可是那天晚上，小艾高兴的一宿没睡，那一晚上，他的笑容都是幸福的。</p><p>可是事情永远不会那么顺利，在香山公园里，当他拿起他买的情侣戒指送给晨晨时，晨晨没有接受，说了一堆他也没有听进去的话，就这样，又开始了分离，而谁也不知道这次分离竟然时一辈子的再也不见。</p><p>后来的后来，他又遇见了别的女孩，不知道是不是因为后来的女孩都像小艾记忆里的莫晨。只知道，他都很珍惜。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。&lt;br&gt;
    
    </summary>
    
      <category term="夏未眠" scheme="http://yoursite.com/categories/%E5%A4%8F%E6%9C%AA%E7%9C%A0/"/>
    
    
      <category term="夏未眠" scheme="http://yoursite.com/tags/%E5%A4%8F%E6%9C%AA%E7%9C%A0/"/>
    
      <category term="青春" scheme="http://yoursite.com/tags/%E9%9D%92%E6%98%A5/"/>
    
  </entry>
  
  <entry>
    <title>这夏未眠.序</title>
    <link href="http://yoursite.com/2016/09/15/%E5%A4%8F%E6%9C%AA%E7%9C%A0/%E8%BF%99%E5%A4%8F%E6%9C%AA%E7%9C%A0-%E5%BA%8F/"/>
    <id>http://yoursite.com/2016/09/15/夏未眠/这夏未眠-序/</id>
    <published>2016-09-15T10:08:51.000Z</published>
    <updated>2017-11-11T10:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。<br><a id="more"></a><br>那个时候，还没有遇到你所想遇到的人，或许回忆还沉淀在高中的时光里，或是幸福，或是苦涩，或是幸福之后的苦涩，回过头来，看着满屋子的学长学姐，心里是及其复杂的，有种说不出的难过，那时候我是不是在想，现在的你（们）会在哪里念大学呢？</p><p>想着想着眼角便淌出了泪水，我想我的大学要完成一件至少我自己觉得满意的事，于是便有了你现在看到的这个序，不知道是不是受郭敬明的影响，因为我看过他的唯一一本小说，也是我看过的唯一一本小说——《夏至未至》，我想写一本书，或者更准确的说，我想写一个人的青春。</p><p>在13年军训结束之后，我构思了整个体系，定了这本书的名字——《这夏未眠》，熟悉我的朋友，也知道这是我的QQ网名，QQ作为那个时代的记忆，总会残留一些悲伤的故事，于是我到现在四年了，我从没换过QQ网名，或许是害怕，害怕那些好久不联系的朋友，找不到我吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。&lt;br&gt;
    
    </summary>
    
      <category term="夏未眠" scheme="http://yoursite.com/categories/%E5%A4%8F%E6%9C%AA%E7%9C%A0/"/>
    
    
      <category term="夏未眠" scheme="http://yoursite.com/tags/%E5%A4%8F%E6%9C%AA%E7%9C%A0/"/>
    
      <category term="青春" scheme="http://yoursite.com/tags/%E9%9D%92%E6%98%A5/"/>
    
  </entry>
  
</feed>
